import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-pro-image-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        #"GPT-5(ç„¡æ•ˆ)": "models/gpt-5",
        #"GPT-5 Mini(ç„¡æ•ˆ)": "models/gpt-5-mini",
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=0, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å–®ä¸€ä»£ç†æ•´åˆç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        
        specific_rules = []
        general_rules = []
        match_log = []

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            
            # --- è®€å–å·¥ç¨‹æ¬„ä½ ---
            spec = str(row.get('Standard_Spec', ''))
            if str(spec).lower() == 'nan': spec = ""
            
            category = str(row.get('Category', ''))
            if str(category).lower() == 'nan': category = ""
            
            logic = str(row.get('Logic_Prompt', ''))
            if str(logic).lower() == 'nan': logic = ""
            
            # --- è®€å–æœƒè¨ˆä¸‰æ¬„ä½ (æ–°åŠŸèƒ½) ---
            # 1. å–®é …æ ¸å°
            u_local = str(row.get('Unit_Rule_Local', ''))
            if u_local.lower() == 'nan': u_local = ""
            
            # 2. èšåˆçµ±è¨ˆ
            u_agg = str(row.get('Unit_Rule_Agg', ''))
            if u_agg.lower() == 'nan': u_agg = ""
            
            # 3. é‹è²»è¨ˆç®—
            u_freight = str(row.get('Unit_Rule_Freight', ''))
            if u_freight.lower() == 'nan': u_freight = ""
            
            keywords = str(row.get('Trigger_Keywords', ''))
            if str(keywords).lower() == 'nan': keywords = ""
            
            is_general_rule = "(é€šç”¨)" in item_name
            
            # --- æƒ…å¢ƒ A: é€šç”¨è¦å‰‡ ---
            if is_general_rule:
                if not keywords:
                    rule_desc = f"- **[å…¨åŸŸæ†²æ³•] {item_name}**\n  - æŒ‡ä»¤: {logic}"
                    general_rules.append(rule_desc)
                    if debug_mode: match_log.append(f"âš–ï¸ [æ†²æ³•] {item_name} (å¼·åˆ¶è¼‰å…¥)")
                
                elif keywords:
                    cleaned_kws = keywords.replace("ï¼Œ", ",").replace("ã€", ",").split(",")
                    cleaned_kws = [k.strip() for k in cleaned_kws if k.strip()]
                    formatted_keywords = str(cleaned_kws)

                    rule_desc = (
                        f"- **{item_name}**\n"
                        f"  - è§¸ç™¼é—œéµå­—: `{formatted_keywords}`\n"
                        f"  - é‚è¼¯: {logic}"
                    )
                    general_rules.append(rule_desc)
                    if debug_mode: match_log.append(f"ğŸ“š [é€šç”¨] {item_name} (é—œéµå­—: {formatted_keywords})")
            
            # --- æƒ…å¢ƒ B: ç‰¹å®šå°ˆæ¡ˆè¦å‰‡ ---
            else:
                if not item_name: continue
                keyword_clean = item_name.upper().replace(" ", "")
                
                score = fuzz.partial_ratio(keyword_clean, ocr_text_clean)
                threshold = 85
                
                if debug_mode:
                    status_icon = "âœ…" if score >= threshold else "âŒ"
                    match_log.append(f"- {status_icon} **[ç‰¹è¦] {item_name}** | åˆ†æ•¸: `{score}`")
                
                if score >= threshold:
                    desc = f"- **[ç‰¹å®š] {item_name}**"
                    # å·¥ç¨‹è³‡è¨Š
                    if spec: desc += f"\n  - [å·¥]è¦æ ¼æ¨™æº–: {spec}"
                    if logic: desc += f"\n  - [å·¥]ç‰¹æ®ŠæŒ‡ä»¤: {logic}"
                    if category: desc += f"\n  - [å·¥]åˆ†é¡: {category}"
                    
                    # æœƒè¨ˆè³‡è¨Š (åˆ†é–‹åˆ—å‡ºï¼Œè®“ AI å°è™Ÿå…¥åº§)
                    if u_local:   desc += f"\n  - [æœƒ]å–®é …æ ¸å°è¦å‰‡: **{u_local}**"
                    if u_agg:     desc += f"\n  - [æœƒ]èšåˆçµ±è¨ˆè¦å‰‡: **{u_agg}**"
                    if u_freight: desc += f"\n  - [æœƒ]é‹è²»è¨ˆç®—è¦å‰‡: **{u_freight}**"
                    
                    specific_rules.append(desc)
        
        final_output = ""
        
        if specific_rules:
            final_output += "### ğŸ¯ ç¬¬ä¸€å€ï¼šå°ˆæ¡ˆç‰¹å®šè¦å‰‡ (æœ€é«˜æ¬Šé™)\n" + "\n".join(specific_rules) + "\n\n"
            
        if general_rules:
            final_output += "### ğŸ“š ç¬¬äºŒå€ï¼šé€šç”¨é‚è¼¯è³‡æ–™åº« (åŸºç¤é‚è¼¯)\n"
            final_output += "\n".join(general_rules)
            
        if not final_output:
            final_output = "ç„¡ç‰¹å®šè¦å‰‡ã€‚"

        if debug_mode:
            final_output += "\n\n---\n### ğŸ•µï¸â€â™‚ï¸ è¦å‰‡åŒ¹é…æ—¥èªŒ (Match Log)\n" + "\n".join(match_log)
            
        return final_output

    except Exception as e:
        return f"è®€å–è¦å‰‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"

# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            markdown_output += f"\n### Table {idx + 1} (Page {page_num}):\n"
            rows = {}
            stop_processing_table = False 
            
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"
    
    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, None, real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data

# --- 5. ç¸½ç¨½æ ¸ Agent (æ•´åˆç‰ˆ - å¼·é‚è¼¯å„ªåŒ–) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    
    # è®€å–æ‰€æœ‰è¦å‰‡
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€ç¸½ç¨½æ ¸å®˜ã€‘ã€‚
    ä½ å¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£åŸ·è¡Œä»¥ä¸‹é›™æ¨¡çµ„ç¨½æ ¸ï¼Œç¦æ­¢ä»»ä½•ä¸»è§€è§£é‡‹ã€‚

    {dynamic_rules}
    
    ---

    ### âš–ï¸ åˆ¤æ±ºæ†²æ³• (Hierarchy of Authority)
    1. **[ç¬¬ä¸€å€ï¼šå°ˆæ¡ˆç‰¹å®šè¦å‰‡]** ç‚ºæœ€é«˜æº–å‰‡ã€‚
    2. **[ç¬¬äºŒå€ï¼šé€šç”¨é‚è¼¯]** ç‚ºå…¨å» ç‰©ç†æ³•å‰‡ï¼Œé è¨­é–‹å•Ÿï¼Œé™¤éç‰¹è¦å¯«æ˜ã€Œè±å…ã€ã€‚
    3. **[é›™è»Œåˆ¤å®š]**ï¼šæ•¸å€¼è¦æ ¼ç”± Python ç¡¬é‚è¼¯åˆ¤å®šï¼›çµ±è¨ˆåŠ ç¸½èˆ‡ç‰©ç†æµç¨‹ç”± AI åˆ¤å®šã€‚

    ---

    ### ğŸš€ åŸ·è¡Œç¨‹åº (Execution Procedure)

    #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹å°ºå¯¸æå– (ä¾›ç³»çµ±è¤‡æ ¸)
    è«‹ç²¾ç¢ºæŠ„éŒ„å„é æ•¸æ“šã€‚**åš´ç¦è·¨é è…¦è£œï¼ŒåªæŠ„éŒ„ç•¶å‰é é¢æ•¸å­—ã€‚**
    1. **è§£ææ¨™æº–**ï¼š
       - **std_max**: æå–å–®ä¸€é–€æª»å€¼ï¼ˆå¦‚ï¼šè‡³ 196mm ç‚ºæ­¢ï¼‰ã€‚
       - **std_list**: åˆ—è¡¨ã€‚æå–æ‰€æœ‰ç¨ç«‹ä¸Šé™ï¼ˆå¦‚ï¼š143, 163ï¼‰ã€‚**åš´ç¦**å°‡å¯¦æ¸¬æ•¸æ“šèª¤å…¥æ­¤å€ã€‚
       - **std_ranges**: åˆ—è¡¨ä¹‹åˆ—è¡¨ã€‚è‹¥æœ‰ `Â±` æˆ–åå·®ï¼ˆå¦‚ 200+0.5ï¼‰ï¼Œ**è«‹ AI å…ˆè¡Œè¨ˆç®—å‡ºæœ€çµ‚ç¯„åœ** [min, max]ã€‚
       - **âš ï¸ éŠ²è£œ/åŠ è‚‰**ï¼šéŠ²è£œè£½ç¨‹å°ºå¯¸å¢åŠ æ˜¯ç‰©ç†æ­£å¸¸çš„ï¼Œåš´ç¦ä»¥æ­¤å ±ã€Œæµç¨‹ç•°å¸¸ã€ã€‚
    2. **åˆ†é¡åˆ†é¡ (category)**ï¼š
       - æ¨™é¡Œå«ã€Œæœªå†ç”Ÿã€ä¸”ä¸å«ã€Œè»¸é ¸ã€ -> `æœªå†ç”Ÿæœ¬é«”`
       - æ¨™é¡Œå«ã€Œæœªå†ç”Ÿã€ä¸”å«ã€Œè»¸é ¸ã€ -> `è»¸é ¸æœªå†ç”Ÿ`
       - æ¨™é¡Œå«ã€ŒéŠ²è£œã€ -> `éŠ²è£œ`
       - å…¶é¤˜ï¼ˆå†ç”Ÿã€ç ”ç£¨ã€ç²¾åŠ å·¥ã€çµ„è£ï¼‰ -> `ç²¾åŠ å·¥å†ç”Ÿ`

    #### ğŸ’° æ¨¡çµ„ Bï¼šæœƒè¨ˆæ•¸é‡èˆ‡ç‰©ç†æµç¨‹ç¨½æ ¸ (ç”± AI åˆ¤å®š)
    1. **å–®é …è¨ˆç®—**ï¼šæ ¸å°æ‹¬è™Ÿå…§ PC æ•¸èˆ‡å…§æ–‡è¡Œæ•¸ã€‚æœ¬é«”å»é‡ï¼Œè»¸é ¸æ¯ç·¨è™Ÿæœ€å¤š2æ¬¡ã€‚
    2. **ç¸½è¡¨åŠ ç¸½ (Global Check)**ï¼š
       - **èšåˆæ¨¡å¼**ï¼šè‹¥æ¨™é¡Œå«ã€Œæ©ŸROLLè»Šä¿®/éŠ²è£œ/æ‹†è£ã€ï¼ŒåŸ·è¡Œ Sum(æœ¬é«”+è»¸é ¸)ã€‚
       - **æ¨™æº–æ¨¡å¼**ï¼šå…¶é¤˜é …ç›®åƒ…åŠ ç¸½åç¨±å°æ‡‰çš„å­é …ã€‚
    3. **ç‰©ç†é †åºèˆ‡ä¾è³´**ï¼šè»Šä¿®æ‡‰ã€Œå‰æ®µ >= å¾Œæ®µã€ï¼›éŠ²è£œæ‡‰ã€Œå‰æ®µ <= å¾Œæ®µã€ã€‚

    ---

    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSONã€‚ç•°å¸¸çµ±è¨ˆå¿…é ˆã€Œé€è¡Œæ‹†åˆ†ã€ä¾†æºé …ç›®èˆ‡é ç¢¼ã€‚

    {{
      "job_no": "å·¥ä»¤ç·¨è™Ÿ",
      "issues": [ 
         {{
           "page": "é ç¢¼",
           "item": "é …ç›®åç¨±",
           "issue_type": "çµ±è¨ˆä¸ç¬¦ / æµç¨‹ç•°å¸¸",
           "common_reason": "å¤±æ•—åŸå›  (15å­—å…§)",
           "failures": [
              {{ "id": "ğŸ” çµ±è¨ˆç¸½å¸³åŸºæº–", "val": "ç›®æ¨™æ•¸", "calc": "ç›®æ¨™" }},
              {{ "id": "é …ç›®å…¨å (P.é ç¢¼)", "val": "è¨ˆæ•¸", "calc": "è¨ˆå…¥åŠ ç¸½" }},
              {{ "id": "ğŸ§® å…§æ–‡å¯¦éš›åŠ ç¸½", "val": "ç¸½è¨ˆ", "calc": "è¨ˆç®—" }}
           ]
         }}
      ],
      "dimension_data": [
         {{
           "page": "æ•¸å­—",
           "item_title": "é …ç›®å…¨å",
           "category": "åˆ†é¡åç¨±",
           "std_max": "æ•¸å­—", 
           "std_list": [],
           "std_ranges": [],
           "std_spec": "åŸå§‹è¦æ ¼æ–‡å­—",
           "data": [ {{ "id": "æ»¾è¼ªç·¨è™Ÿ", "val": "å¯¦æ¸¬å€¼(å­—ä¸²ï¼Œç¦è®Šå‹•ä½æ•¸)" }} ]
         }}
      ]
    }}
    """
    ### âš–ï¸ åˆ¤æ±ºæ†²æ³• (Hierarchy of Authority)
    **è«‹æ³¨æ„ï¼šåˆ¤å®šæ¨™æº–åˆ†ç‚ºã€Œæ•¸æ“šå±¤ã€èˆ‡ã€Œé‚è¼¯å±¤ã€ï¼Œå…©è€…å¿…é ˆåŒæ™‚æˆç«‹ã€‚**

    **ç¬¬ 1 éšç´šï¼š[ç¬¬ä¸€å€ï¼šå°ˆæ¡ˆç‰¹å®šè¦å‰‡] (Specific Data)**
    *   **æ¬ŠåŠ›**ï¼šå®šç¾©è©²é …ç›®çš„ **ã€Œç›®æ¨™æ•¸å€¼ã€**ã€‚è‹¥æœ‰æ•¸å€¼ï¼Œä»¥æ­¤ç‚ºæº–ã€‚
    *   **æŒ‡ä»¤**ï¼šè‹¥ `ç‰¹æ®ŠæŒ‡ä»¤(Logic)` ç‚ºç©ºï¼Œä»£è¡¨ **ã€Œå®Œå…¨éµå®ˆé€šç”¨é‚è¼¯ã€**ã€‚

    **ç¬¬ 2 éšç´šï¼š[ç¬¬äºŒå€ï¼šé€šç”¨é‚è¼¯è³‡æ–™åº«] (General Logic)**
    *   **æ¬ŠåŠ›**ï¼šå®šç¾©å…¨å» é€šç”¨çš„ **ã€Œç‰©ç†æ³•å‰‡ã€** (å¦‚é †åºã€ä¾è³´æ€§)ã€‚
    *   **å¼·åˆ¶æ€§**ï¼š**é è¨­ç‚ºé–‹å•Ÿç‹€æ…‹**ã€‚é™¤éç¬¬ 1 éšç´šæ˜ç¢ºå¯«å‡ºã€Œè±å…ã€ï¼Œå¦å‰‡ä¸å¯é—œé–‰ã€‚

    ---

    ### ğŸš€ åŸ·è¡Œç¨‹åº (Execution Procedure)

   #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹å°ºå¯¸åŠæµç¨‹ç¨½æ ¸ï¼ˆEngineering Dimensions and Process Auditï¼‰

    **Step 1. å°‡å„é è¡¨æ ¼ä¸­çš„æ•¸å€¼æŠ„éŒ„ç‚ºçµæ§‹åŒ–æ•¸æ“šï¼Œä¸¦ç”±ä½ ã€Œå…ˆè¡Œè§£æã€è¦æ ¼æ–‡å­—ï¼š
    
    A. **è§£æåˆ¤å®šé–€æª» (std_max / std_list)**ï¼š
       - **std_max**: æå–è¦æ ¼ä¸­çš„ã€Œé–€æª»å€¼ã€ã€‚è‹¥è¦æ ¼ç‚ºã€Œè‡³ 196mm å†ç”Ÿã€ï¼Œæå– `196.0`ã€‚
       - **std_list**: åˆ—è¡¨ã€‚æå–è¦æ ¼ä¸­å‡ºç¾çš„æ‰€æœ‰ç¨ç«‹å°ºå¯¸ï¼ˆå¦‚è»¸é ¸è¦æ ¼ 143, 163ï¼‰ã€‚
       - **âš ï¸ æ’é™¤åŠ å·¥é‡**ï¼šåš´ç¦æå–ã€Œæ¯æ¬¡è»Šä¿® 0.5~2mmã€é€™é¡éç¨‹æ•¸å­—ï¼Œåƒ…æå–ç›®æ¨™å°ºå¯¸ã€‚

    B. **è§£æç›®æ¨™å€é–“ (std_ranges)**ï¼š
       - **å¤šé‡å€é–“**ï¼šè‹¥è¦æ ¼ç‚ºã€Œ135~129ã€ï¼Œæå–ç‚º `[[129.0, 135.0]]`ã€‚
       - **å…¬å·®è¨ˆç®—**ï¼šè‹¥è¦æ ¼å« `Â±`ï¼Œè«‹ä½ å…ˆè¨ˆç®—å‡ºçµæœã€‚å¦‚ã€Œ200 Â± 0.5ã€æå–ç‚º `[[199.5, 200.5]]`ã€‚
       - **åå·®è¨ˆç®—**ï¼šå¦‚ã€Œ200 +0.3/-0.1ã€æå–ç‚º `[[199.9, 200.3]]`ã€‚

    C. **åˆ†é¡è­˜åˆ¥ (category)**ï¼š
       - æ¨™é¡Œå«ã€Œæœªå†ç”Ÿã€ä¸”ä¸å«ã€Œè»¸é ¸ã€ -> `æœªå†ç”Ÿæœ¬é«”`
       - æ¨™é¡Œå«ã€Œæœªå†ç”Ÿã€ä¸”å«ã€Œè»¸é ¸ã€ -> `è»¸é ¸æœªå†ç”Ÿ`
       - æ¨™é¡Œå«ã€Œå†ç”Ÿã€ã€ã€Œç²¾åŠ å·¥ã€ã€ã€Œç ”ç£¨ã€ã€ã€Œè»Šä¿®åŠ å·¥ã€ã€ã€Œçµ„è£ã€ -> `ç²¾åŠ å·¥å†ç”Ÿ`
       - æ¨™é¡Œå«ã€ŒéŠ²è£œã€ -> `éŠ²è£œ`

    **Step 2. ç‰©ç†æµç¨‹æª¢æŸ¥ (AI ç¹¼çºŒåŸ·è¡Œ)**ï¼š
    *   **ç‰©ç†é †åº**ï¼šé‡å°åŒä¸€ç·¨è™Ÿï¼Œæª¢æŸ¥è£½ç¨‹æ¼”é€²ã€‚åŸå‰‡ï¼š`å‰æ®µ(æœªå†ç”Ÿ) <= å¾Œæ®µ(å†ç”Ÿ/éŠ²è£œ/ç ”ç£¨)`ã€‚
    *   **æµç¨‹ä¾è³´**ï¼šæª¢æŸ¥å¾Œæ®µè£½ç¨‹æ˜¯å¦æœ‰å°æ‡‰çš„å‰æ®µç´€éŒ„ã€‚è‹¥æµç¨‹ä¸­æ–·ï¼Œåœ¨ `issues` ä¸­å›å ±ã€‚
    
    ### ğŸš€ åŸ·è¡Œæ¨¡çµ„ Bï¼šæœƒè¨ˆæ•¸é‡æ ¸å° (ä¸‰éšæ®µç¨ç«‹åƒæ•¸)
    **è«‹æ³¨æ„ï¼šæœƒè¨ˆæª¢æŸ¥åˆ†ç‚ºä¸‰å€‹ç¨ç«‹æ­¥é©Ÿï¼Œæ¯å€‹æ­¥é©Ÿå¿…é ˆåƒè€ƒ Excel å°æ‡‰çš„è¦å‰‡æ¬„ä½ã€‚**
    
    **Step 1: å–®é …æ•¸é‡è¨ˆç®— (Local Calculation)**
    *   **ç®—æ³•**ï¼šé …ç›®è¨ˆæ•¸ï¼ˆç›®æ¨™æ•¸ï¼‰ = åˆ—è¡¨çš„"ç·¨è™Ÿ"å€‹æ•¸ã€‚
        ä¾‹ï¼šè¦ç¯„æ¨™æº–ï¼šW3 #6 295ï¼ˆXï¼‰ ROLL æœ¬é«”æœªå†ç”Ÿè»Šä¿®ï¼ˆ12PCï¼‰ï¼Œæ­¤é …ç›®å¾Œè¦æœ‰12å€‹ç·¨è™Ÿã€‚
        *   **æœ¬é«” (Body)**: ä½¿ç”¨ `Count Distinct` (å»é‡è¨ˆç®—ç¨ç«‹ç·¨è™Ÿ)ã€‚
        *   **è»¸é ¸/å…§å­”**: ä½¿ç”¨ `Count Total Rows` (è¨ˆç®—ç¸½è¡Œæ•¸)ï¼Œé …ç›®å…§æ¯å€‹ç¨ç«‹ç·¨è™Ÿ**ä¸å¯é‡è¤‡è¶…é2æ¬¡**ã€‚
        *   **åƒæ•¸ä¾†æº**ï¼šæŸ¥çœ‹ç‰¹è¦çš„ **`[æœƒ]å–®é …æ ¸å°è¦å‰‡`**ã€‚
        *   è‹¥æœ‰ (å¦‚ "1SET=4PCS")ï¼šä»¥æ­¤ç‚ºæº–è¨ˆç®— (Rows / 4)ã€‚
        *   è‹¥ç„¡ï¼šé è¨­ `1 SET = 2 PCS`, `1 PC = 1 PC`ã€‚
        
    **Step 2: ç¸½è¡¨æ ¸å° (Global Summary Check)**
    *   **ç›®æ¨™**ï¼šæ ¸å°å·¦ä¸Šè§’ã€Œå¯¦äº¤æ•¸é‡ã€ vs ã€Œè·¨é å…§æ–‡é …ç›®åŠ ç¸½ã€ã€‚
    *   **åŸ·è¡Œé‚è¼¯**ï¼šè«‹å…ˆè®€å–å·¦ä¸Šè§’çš„ã€Œé …ç›®åç¨±ã€ï¼Œä¾æ“šä¸‹åˆ—è¦å‰‡æ±ºå®šå“ªäº›ã€Œå…§æ–‡é …ç›®ã€éœ€è¦è¢«åŠ ç¸½ï¼š
    *   **è­‰æ“šæ”¶é›†**ï¼šè‹¥ç™¼ç¾æ•¸é‡ä¸ç¬¦ï¼Œä½ å¿…é ˆåˆ—å‡ºæ‰€æœ‰åƒèˆ‡åŠ ç¸½çš„ã€Œè­‰æ“šæ¸…å–®ã€ã€‚
        - æ ¼å¼ï¼š`é …ç›®åç¨± (Page é ç¢¼)`
        - æ•¸å€¼ï¼šè©²é …ç›®çš„è¨ˆæ•¸çµæœã€‚

    **A. é›™è»Œèšåˆæ¨¡å¼ (Aggregated Mode)**
    *   **è§¸ç™¼æ¢ä»¶**ï¼šç•¶å·¦ä¸Šè§’é …ç›®åç¨± **åŒ…å«**ã€Œæ©ŸROLLè»Šä¿®ã€ã€ã€Œæ©ŸROLLéŠ²è£œã€ã€ã€Œæ©ŸROLLæ‹†è£ã€å…¶ä¸­ä¹‹ä¸€æ™‚ã€‚
        *   *(ä¾‹å¦‚ï¼š"W3 #1æ©Ÿ ROLL è»Šä¿®", "ROLL éŠ²è£œ")*
        **åŠ ç¸½æ¸…å–®**ï¼šå¿…é ˆæ˜ç¢ºåˆ—å‡ºå„é è¢«è¨ˆå…¥çš„å­é …ï¼ˆå¦‚ï¼šæœ¬é«”æœªå†ç”Ÿ(P.2): 5, è»¸é ¸å†ç”Ÿ(P.3): 2...ï¼‰ã€‚
    *   **åŠ ç¸½ç¯„åœ (é è¨­)**ï¼š
        *   **æ©ŸROLLè»Šä¿®** = Sum(æœ¬é«”æœªå†ç”Ÿ + æœ¬é«”å†ç”Ÿ + è»¸é ¸æœªå†ç”Ÿ + è»¸é ¸å†ç”Ÿ)
        *   **æ©ŸROLLéŠ²è£œ** = Sum(æœ¬é«”éŠ²è£œ + è»¸é ¸éŠ²è£œ)
        *   **æ©ŸROLLæ‹†è£** = Sum(æ–°å“çµ„è£ + èˆŠå“æ‹†è£)
    *   **ä¾‹å¤–éæ¿¾ (ç‰¹è¦ä»‹å…¥)**ï¼š
        *   åœ¨åŠ ç¸½ä¸Šè¿°é …ç›®ä¹‹å‰ï¼Œ**å¿…é ˆ**æª¢æŸ¥è©²é …ç›®çš„ **`[æœƒ]èšåˆçµ±è¨ˆè¦å‰‡`**ã€‚
        *   è‹¥å¯« **"è±å…"** æˆ– **"å¼·åˆ¶æ­¸é¡ç‚ºé€šç”¨"**ï¼šâŒ **åš´ç¦**å°‡å…¶åŠ å…¥ä¸Šè¿°ç¸½å¸³ã€‚
        *   è‹¥å¯« **"1SET=1PC"**ï¼šâš ï¸ åƒ…åŠ å…¥ **1** å€‹å–®ä½ (è€Œéå…§æ–‡çš„å¯¦éš›è¡Œæ•¸)ã€‚

    **B. æ¨™æº–å°æ‡‰æ¨¡å¼ (Standard Mode)**
    *   **è§¸ç™¼æ¢ä»¶**ï¼šç•¶å·¦ä¸Šè§’é …ç›®åç¨± **ä¸åŒ…å«**ã€Œæ©ŸROLLè»Šä¿®ã€ã€ã€Œæ©ŸROLLéŠ²è£œã€ã€ã€Œæ©ŸROLLæ‹†è£ã€å…¶ä¸­ä¹‹ä¸€æ™‚ã€‚
    *   **åŠ ç¸½æ¸…å–®**ï¼šåˆ—å‡ºæ‰€æœ‰åç¨±å°æ‡‰çš„å­é …åŠå…¶æ‰€åœ¨é æ•¸ã€‚
    *   **åŠ ç¸½ç¯„åœ**ï¼šåƒ…åŠ ç¸½å…§æ–‡ä¸­ **ã€Œåç¨±å®Œå…¨å°æ‡‰ã€** æˆ– **ã€Œé‚è¼¯ä¸Šå±¬æ–¼è©²é …ç›®ã€** çš„å­é …ç›®ã€‚
    *   **é‚è¼¯**ï¼šæ­¤æ¨¡å¼ä¸‹ï¼Œ**å¿½ç•¥** Excel çš„ `[æœƒ]èšåˆçµ±è¨ˆè¦å‰‡`ã€‚åªè¦åç¨±å°ä¸Šï¼Œå°±ç›´æ¥åŠ ç¸½ã€‚

    **Step 3: é‹è²»è¨ˆç®— (Freight Check)
    *   **ä»»å‹™**ï¼šè¨ˆç®—å…¨å·ã€Œæœ¬é«”ã€çš„ã€Œæœªå†ç”Ÿè»Šä¿®ã€ç¸½æ•¸ï¼Œæ ¸å°å·¦ä¸Šè§’é‹è²»é …æ¬¡ç¸½æ•¸ã€‚
    *   **åƒæ•¸ä¾†æº**ï¼šæŸ¥çœ‹ç‰¹è¦çš„ **`[æœƒ]é‹è²»è¨ˆç®—è¦å‰‡`**ã€‚
        *   è‹¥å¯« **"è±å…"**ï¼š**åš´ç¦**å°‡æ­¤é …ç›®è¨ˆå…¥é‹è²»ã€‚
        *   è‹¥å¯« **"1SET=1PC"**ï¼šä»¥ 1:1 ç´¯åŠ è‡³é‹è²»ã€‚
        *   è‹¥ç„¡ï¼šé è¨­ä¾æ“š Step 1 çš„çµæœç´¯åŠ ã€‚

    ---
    
   ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    å¿…é ˆå›å‚³å–®ä¸€ JSON ç‰©ä»¶ï¼ŒåŒ…å« `issues` (ç•°å¸¸å›å ±) èˆ‡ `dimension_data` (æ•¸æ“šæŠ„éŒ„)ã€‚

    #### 1. ç•°å¸¸å›å ±å€ (`issues`) - ã€æœƒè¨ˆèˆ‡æµç¨‹ã€‘
    - **å°è±¡**ï¼šæœƒè¨ˆæ•¸é‡ã€ç‰©ç†æµç¨‹é †åºã€å¹½éˆå·¥ä»¶ã€é‹è²»æ ¸å°ã€‚
    - **çµ±è¨ˆæ‹†åˆ†è¦å‰‡ (æ¥µé‡è¦)**ï¼šåš´ç¦åˆä½µæ˜ç´°ã€‚æ¯ä¸€å€‹ä¾†æºé …ç›®/é é¢å¿…é ˆæ˜¯ `failures` ä¸­çš„ç¨ç«‹ç‰©ä»¶ã€‚
    - **ç¯„ä¾‹æ ¼å¼**ï¼š
        "failures": [
          {{ "id": "ğŸ” çµ±è¨ˆç¸½å¸³åŸºæº–", "val": "20", "calc": "ç¸½è¡¨ç›®æ¨™å€¼" }},
          {{ "id": "é …ç›®åç¨± (P.3)", "val": "8", "calc": "è¨ˆå…¥åŠ ç¸½" }},
          {{ "id": "ğŸ§® å…§æ–‡å¯¦éš›åŠ ç¸½", "val": "è¨ˆç®—æ•¸", "calc": "è¨ˆç®—ç¸½é‡" }}
        ]

    #### 2. æ•¸æ“šæå–å€ (`dimension_data`) - ã€Pythonç¡¬æ ¸è¤‡æ ¸ç”¨ã€‘
    - **std_list**: è¦æ ¼ä¸­å‡ºç¾çš„æ‰€æœ‰å–®ä¸€æ•¸å­—åˆ—è¡¨ã€‚
    - **std_ranges**: è‹¥è¦æ ¼å« `Â±` æˆ–åå·®ï¼Œè«‹ AI **å…ˆè¡Œè¨ˆç®—**å‡ºæœ€çµ‚ [min, max] å€é–“åˆ—è¡¨ã€‚
    - **category**: [æœªå†ç”Ÿæœ¬é«”, è»¸é ¸æœªå†ç”Ÿ, ç²¾åŠ å·¥å†ç”Ÿ, éŠ²è£œ, çµ„è£]

    ---
    {{
      "job_no": "å·¥ä»¤ç·¨è™Ÿ",
      "issues": [ 
         {{
           "page": "é ç¢¼",
           "item": "é …ç›®åç¨±",
           "issue_type": "çµ±è¨ˆä¸ç¬¦ / æµç¨‹ç•°å¸¸",
           "common_reason": "å¤±æ•—åŸå› ",
           "failures": [] 
         }}
      ],
      "dimension_data": [
         {{
           "page": "æ•¸å­—",
           "item_title": "é …ç›®å®Œæ•´åç¨±",
           "category": "åˆ†é¡åç¨±",
           "std_max": "é–€æª»å€¼(æ•¸å­—)", 
           "std_list": [],
           "std_ranges": [],
           "std_spec": "åŸå§‹è¦æ ¼æ–‡å­—",
           "data": [ {{ "id": "æ»¾è¼ªç·¨è™Ÿ", "val": "å¯¦æ¸¬å€¼(å­—ä¸²ï¼Œç¦æ­¢æ›´å‹•ä½æ•¸)" }} ]
         }}
      ]
    }}
    
    """
    
    generation_config = {"response_mime_type": "application/json", "temperature": 0.0, "top_k": 1, "top_p": 0.95}
    
    try:
        # === åˆ†æµ A: Google Gemini ===
        if "gemini" in model_name.lower():
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            response = model.generate_content([system_prompt, combined_input], generation_config=generation_config)
            
            raw_content = response.text
            usage_meta = response.usage_metadata
            usage_in = usage_meta.prompt_token_count if usage_meta else 0
            usage_out = usage_meta.candidates_token_count if usage_meta else 0

        # === åˆ†æµ B: OpenAI GPT ===
        else:
            if not OPENAI_KEY:
                return {"job_no": "Error", "issues": [{"item": "Error", "common_reason": "ç¼ºå°‘ OpenAI Key"}], "_token_usage": {"input":0, "output":0}}
            
            client = OpenAI(api_key=OPENAI_KEY)
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": combined_input}
                ],
                temperature=0.0
            )
            raw_content = response.choices[0].message.content
            usage_in = response.usage.prompt_tokens
            usage_out = response.usage.completion_tokens

        # =========================================================
        # ğŸ›¡ï¸ çµ•å°é˜²ç¦¦ï¼šJSON è§£æèˆ‡çµæ§‹é‡å»º
        # =========================================================
        
        # 1. æ¸…æ´— Markdown
        if "```json" in raw_content:
            raw_content = raw_content.replace("```json", "").replace("```", "")
        elif "```" in raw_content:
            raw_content = raw_content.replace("```", "")
            
        # 2. å˜—è©¦è§£æ
        try:
            parsed_data = json.loads(raw_content)
        except:
            parsed_data = {"job_no": "JSON Error", "issues": []}

        # 3. å»ºæ§‹æœ€çµ‚å›å‚³ç‰©ä»¶
        final_response = {}

        if isinstance(parsed_data, dict):
            final_response = parsed_data
        elif isinstance(parsed_data, list):
            final_response = {"job_no": "Unknown", "issues": parsed_data}
        else:
            final_response = {"job_no": "Unknown", "issues": []}

        # 4. è£œå…¨å¿…è¦æ¬„ä½
        if "issues" not in final_response:
            final_response["issues"] = []
        if "job_no" not in final_response:
            final_response["job_no"] = "Unknown"

        # 5. ã€ä¿®æ”¹é»ã€‘åƒåœ¾éæ¿¾å™¨ (Garbage Collector) & çŸ›ç›¾æ¸…æ´—
        valid_issues = []
        for i in final_response["issues"]:
            if isinstance(i, dict):
                item_name = i.get("item", "")
                reason = i.get("common_reason", "")
                i_type = i.get("issue_type", "")

                # 1. åŸºæœ¬é˜²å‘†ï¼šæ²’æœ‰ item åç¨±å°±è¸¢æ‰
                if not item_name: 
                    continue
                    
                # 2. ã€é—œéµä¿®æ­£ã€‘çŸ›ç›¾æ¸…æ´—
                # å¦‚æœ AI èªªã€Œåˆæ ¼ã€ï¼Œä½†é€™åˆä¸æ˜¯ã€ŒæœªåŒ¹é…è¦å‰‡ã€çš„å¼·åˆ¶å›å ± -> ä»£è¡¨é€™æ˜¯ AI å¤šå˜´ï¼Œè¸¢æ‰ï¼
                if "åˆæ ¼" in reason and "æœªåŒ¹é…" not in i_type:
                     continue
                
                # 3. å¦‚æœ AI èªªã€Œåˆæ ¼ã€ï¼Œä¸”æ˜¯ã€ŒæœªåŒ¹é…ã€ï¼Œä½† issue_type å»å¯«ã€Œæ•¸å€¼è¶…è¦ã€ -> å¼·åˆ¶ä¿®æ­£é¡å‹
                if "åˆæ ¼" in reason and "æœªåŒ¹é…" in i_type:
                    i["issue_type"] = "âš ï¸æœªåŒ¹é…è¦å‰‡" # å¼·åˆ¶ä¿®æ­£ç‚ºé»ƒè‰²è­¦å‘Š

                valid_issues.append(i)
        
        # å°‡æ¸…æ´—å¾Œçš„ä¹¾æ·¨æ¸…å–®æ”¾å›å»
        final_response["issues"] = valid_issues

        # 6. æ³¨å…¥ Token ç”¨é‡
        final_response["_token_usage"] = {"input": usage_in, "output": usage_out}
        
        return final_response

    except Exception as e:
        # é€™å€‹ except å¿…é ˆå°é½Šä¸Šé¢çš„ try
        return {"job_no": "Error", "issues": [{"item": "System Error", "common_reason": str(e)}], "_token_usage": {"input": 0, "output": 0}}

# --- agent_unified_check çš„çµå°¾ ---
        final_response["_token_usage"] = {"input": usage_in, "output": usage_out}
        return final_response

    except Exception as e:
        return {"job_no": "Error", "issues": [{"item": "System Error", "common_reason": str(e)}], "_token_usage": {"input": 0, "output": 0}}

def python_numerical_audit(dimension_data):
    new_issues = []
    import re
    if not dimension_data: return new_issues

    for item in dimension_data:
        rid_list = item.get("data", [])
        title = item.get("item_title", "")
        cat = item.get("category", "")
        page_num = item.get("page", "?")
        raw_spec = str(item.get("std_spec", ""))
        
        # --- ğŸ›¡ï¸ æ•¸æ“šæ¸…æ´—æ¿¾é¡ï¼šéæ¿¾æ©Ÿè™Ÿã€å‹è™Ÿé›œè¨Š ---
        all_raw_nums = [float(n) for n in re.findall(r"\d+\.?\d*", raw_spec)]
        # æ¿¾æ‰ 1,2,3,4,6 è™Ÿæ©Ÿï¼Œæ¿¾æ‰å¸¸è¦‹è¼¥è¼ªå‹è™Ÿ 300, 350
        noise = [1.0, 2.0, 3.0, 4.0, 6.0, 300.0, 350.0]
        clean_std = [n for n in all_raw_nums if n not in noise and n > 5] # å°æ–¼5çš„é€šå¸¸æ˜¯åŠ å·¥é‡ï¼Œæ’é™¤

        for entry in rid_list:
            rid = entry.get("id")
            val_str = str(entry.get("val", "")).strip()
            if not val_str or val_str in ["N/A", "nan", ""]: continue

            try:
                val = float(val_str)
                is_pure_int = "." not in val_str
                is_two_dec = "." in val_str and len(val_str.split(".")[-1]) == 2
                is_passed = True
                reason = ""
                target_used = "N/A"

                # --- 1. æœªå†ç”Ÿæœ¬é«” (æœ€å¤§å€¼åŸºæº– + ä¸‰æº–å‰‡) ---
                if cat == "æœªå†ç”Ÿæœ¬é«”":
                    target_used = max(clean_std) if clean_std else 196.0
                    if val <= target_used:
                        if not is_pure_int:
                            is_passed, reason = False, f"æœªå†ç”Ÿ(<=æ¨™æº–{target_used}): æ‡‰ç‚ºæ•´æ•¸"
                    else: # val > target
                        if is_two_dec: is_passed = True
                        elif is_pure_int: is_passed, reason = False, f"æœªå†ç”Ÿ(>æ¨™æº–{target_used}): è¶…è¦ç¦å¡«æ•´æ•¸ï¼Œæ‡‰å¡«å…©ä½å°æ•¸"
                        else: is_passed, reason = False, f"æœªå†ç”Ÿ(>æ¨™æº–{target_used}): æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰ç‚º#.##"

                # --- 2. è»¸é ¸æœªå†ç”Ÿ (æ•´æ•¸ + æœ€å¤§å€¼åŸºæº–) ---
                elif cat == "è»¸é ¸æœªå†ç”Ÿ":
                    target_used = max(clean_std) if clean_std else 0
                    if not is_pure_int:
                        is_passed, reason = False, "è»¸é ¸æœªå†ç”Ÿ: æ‡‰ç‚ºç´”æ•´æ•¸æ ¼å¼"
                    elif target_used > 0 and val > target_used:
                        is_passed, reason = False, f"è»¸é ¸æœªå†ç”Ÿ: è¶…å‡ºä¸Šé™ {target_used}"

                # --- 3. ç²¾åŠ å·¥å†ç”Ÿé¡ (å…©ä½å°æ•¸ + å€é–“åˆ¤æ–·) ---
                elif cat == "ç²¾åŠ å·¥å†ç”Ÿ":
                    if not is_two_dec:
                        is_passed, reason = False, "ç²¾åŠ å·¥: æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰ç‚ºå…©ä½å°æ•¸"
                    elif clean_std:
                        # è™•ç† Â± æˆ– å€é–“ï¼šè‹¥æœ‰å…©å€‹æ•¸å­—å‰‡å– min/max å€é–“
                        if len(clean_std) >= 2:
                            s_min, s_max = min(clean_std), max(clean_std)
                            target_used = f"{s_min}~{s_max}"
                            if not (s_min <= val <= s_max):
                                is_passed, reason = False, f"ç²¾åŠ å·¥: ä¸åœ¨å€é–“å…§ {target_used}"
                        else: # åªæœ‰ä¸€å€‹æ•¸å­—å‰‡è¦–ç‚ºä¸Šé™
                            target_used = clean_std[0]
                            if val > target_used:
                                is_passed, reason = False, f"ç²¾åŠ å·¥: è¶…å‡ºä¸Šé™ {target_used}"

                # --- 4. éŠ²è£œ (æ•´æ•¸ + å°±è¿‘åŒ¹é…åŸºæº–) ---
                elif cat == "éŠ²è£œ":
                    if not is_pure_int:
                        is_passed, reason = False, "éŠ²è£œ: æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰ç‚ºç´”æ•´æ•¸"
                    elif clean_std:
                        # æ ¸å¿ƒåŠŸèƒ½ï¼šå°‹æ‰¾æœ€é è¿‘å¯¦æ¸¬å€¼çš„è¦æ ¼ä½œç‚ºä¸‹é™
                        target_used = min(clean_std, key=lambda x: abs(x - val))
                        if val < target_used:
                            is_passed, reason = False, f"éŠ²è£œä¸è¶³: å¯¦æ¸¬ {val} ä½æ–¼åŒ¹é…åŸºæº– {target_used}"

                if not is_passed:
                    new_issues.append({
                        "page": page_num, "item": title, "issue_type": "æ•¸å€¼ç•°å¸¸(ç³»çµ±åˆ¤å®š)",
                        "rule_used": f"Excel: {raw_spec}", "common_reason": reason,
                        "failures": [{"id": rid, "val": val_str, "target": f"åŸºæº–:{target_used}", "calc": "ğŸ ç³»çµ±ç¡¬æ ¸åˆ¤å®š"}],
                        "source": "ğŸ ç³»çµ±åˆ¤å®š"
                    })
            except: continue
    return new_issues
    
# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”", "ğŸ“Š ä¸Šå‚³ Excel æª”"], 
    horizontal=True
)

with st.container(border=True):
    # --- æƒ…æ³ A: ä¸Šå‚³ç…§ç‰‡ ---
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json' or st.session_state.get('source_mode') == 'excel':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()

    # --- æƒ…æ³ B: ä¸Šå‚³ JSON ---
    elif data_source == "ğŸ“‚ ä¸Šå‚³ JSON æª”":
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                if st.session_state.get('last_loaded_json_name') != current_file_name:
                    json_data = json.load(uploaded_json)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    import re
                    for page in json_data:
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page
                        })
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ JSON: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥ JSONï¼š**{uploaded_json.name}**")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

    # --- æƒ…æ³ C: ä¸Šå‚³ Excel (æ–°å¢çš„æ”¾åœ¨é€™) ---
    elif data_source == "ğŸ“Š ä¸Šå‚³ Excel æª”":
        st.info("ğŸ’¡ ä¸Šå‚³ Excel æª”å¾Œï¼Œç³»çµ±æœƒå°‡è¡¨æ ¼å…§å®¹è½‰æ›ç‚ºæ–‡å­—ä¾› AI ç¨½æ ¸ã€‚")
        uploaded_xlsx = st.file_uploader("ä¸Šå‚³ Excel æª”", type=['xlsx', 'xls'], key="xlsx_uploader")
        
        if uploaded_xlsx:
            try:
                current_file_name = uploaded_xlsx.name
                if st.session_state.get('last_loaded_xlsx_name') != current_file_name:
                    df_dict = pd.read_excel(uploaded_xlsx, sheet_name=None)
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'excel'
                    st.session_state.last_loaded_xlsx_name = current_file_name
                    
                    for sheet_name, df in df_dict.items():
                        df = df.fillna("")
                        md_table = df.to_markdown(index=False)
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': md_table,
                            'header_text': f"ä¾†æºåˆ†é : {sheet_name}",
                            'full_text': f"Excel å…§å®¹ - åˆ†é  {sheet_name}\n" + md_table,
                            'raw_json': None,
                            'real_page': sheet_name
                        })
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥ Excel: {current_file_name}", icon="ğŸ“Š")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“Š ç›®å‰è¼‰å…¥ Excelï¼š**{uploaded_xlsx.name}**")
            except Exception as e:
                st.error(f"Excel è®€å–å¤±æ•—: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        total_start = time.time()
        status = st.empty()
        progress_bar = st.progress(0)
        
        extracted_data_list = [None] * len(st.session_state.photo_gallery)
        full_text_for_search = ""
        total_imgs = len(st.session_state.photo_gallery)
        
        ocr_start = time.time()
        
        def process_image_task(index, item):
            index = int(index)
            # å¦‚æœå·²ç¶“æœ‰è³‡æ–™äº†å°±ä¸é‡è¤‡æƒæ
            if item.get('table_md') and item.get('header_text') and item.get('full_text'):
                real_page = item.get('real_page', str(index + 1))
                return index, item['table_md'], item['header_text'], item['full_text'], None, real_page, None
    
            try:
                if item.get('file') is None:
                    return index, None, None, None, None, None, "ç„¡åœ–ç‰‡æª”æ¡ˆ"
                
                item['file'].seek(0)
                # é€™è£¡æœƒæ¥åˆ°æˆ‘å€‘å‰›æ‰ä¿®æ”¹å¾Œå›å‚³çš„ None
                table_md, header, full, _, real_page = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                return index, table_md, header, full, None, real_page, None
            except Exception as e:
                return index, None, None, None, None, None, f"OCRå¤±æ•—: {str(e)}"

        status.text(f"Azure æ­£åœ¨å¹³è¡Œæƒæ {total_imgs} é æ–‡ä»¶...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for i, item in enumerate(st.session_state.photo_gallery):
                futures.append(executor.submit(process_image_task, i, item))
            
            completed_count = 0
            for future in concurrent.futures.as_completed(futures):
                idx, t_md, h_txt, f_txt, raw_j, r_page, err = future.result()
                idx = int(idx)
                
                if err:
                    st.error(f"ç¬¬ {idx+1} é è®€å–å¤±æ•—: {err}")
                    extracted_data_list[idx] = None
                else:
                    st.session_state.photo_gallery[idx]['table_md'] = t_md
                    st.session_state.photo_gallery[idx]['header_text'] = h_txt
                    st.session_state.photo_gallery[idx]['full_text'] = f_txt
                    st.session_state.photo_gallery[idx]['raw_json'] = raw_j
                    st.session_state.photo_gallery[idx]['real_page'] = r_page
                    st.session_state.photo_gallery[idx]['file'] = None
                    
                    extracted_data_list[idx] = {
                        "page": r_page,
                        "table": t_md or "", 
                        "header_text": h_txt or ""
                    }
                
                completed_count += 1
                progress_bar.progress(completed_count / (total_imgs + 1))
        
        for i, data in enumerate(extracted_data_list):
            if data and isinstance(data, dict):
                page_idx = i
                if 0 <= page_idx < len(st.session_state.photo_gallery):
                    full_text_for_search += st.session_state.photo_gallery[page_idx].get('full_text', '')

        ocr_end = time.time()
        ocr_duration = ocr_end - ocr_start

        combined_input = "ä»¥ä¸‹æ˜¯å„é è³‡æ–™ï¼š\n"
        for i, data in enumerate(extracted_data_list):
            if data is None: continue
            page_num = data.get('page', i+1)
            table_text = data.get('table', '')
            header_text = data.get('header_text', '')
            combined_input += f"\n=== Page {page_num} ===\nã€é é¦–ã€‘:\n{header_text}\nã€è¡¨æ ¼ã€‘:\n{table_text}\n"
            
        status.text("ç¸½ç¨½æ ¸ Agent æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...")
        
        # --- å–®ä¸€ä»£ç†åŸ·è¡Œ ---
        t0 = time.time()
        # å‘¼å«åˆä½µå¾Œçš„ Agent
        res_main = agent_unified_check(combined_input, full_text_for_search, GEMINI_KEY, main_model_name)
        
        # --- âœ¨ æ–°å¢é€™å…©è¡Œï¼šå•Ÿå‹• Python ç¡¬æ ¸è¤‡æ ¸ ---
        dim_data = res_main.get("dimension_data", [])
        python_numeric_issues = python_numerical_audit(dim_data)
        # ----------------------------------------
        
        t1 = time.time()
        time_main = t1 - t0
        
        progress_bar.progress(100)
        status.empty()
        
        total_end = time.time()
        
        # --- 1. æˆæœ¬è¨ˆç®— (ä¿æŒåŸæ¨£) ---
        usage_main = res_main.get("_token_usage", {"input": 0, "output": 0})
        
        def get_model_rate(model_name):
            name = model_name.lower()
            if "gpt" in name:
                if "mini" in name: return 0.15, 0.60
                elif "3.5" in name: return 0.50, 1.50
                else: return 2.50, 10.00
            else:
                if "flash" in name: return 0.075, 0.30
                else: return 1.25, 5.00 # Pro

        rate_in, rate_out = get_model_rate(main_model_name)
        cost_usd = (usage_main["input"] / 1_000_000 * rate_in) + (usage_main["output"] / 1_000_000 * rate_out)
        cost_twd = cost_usd * 32.5
        
        # --- 2. å•Ÿå‹• Python ç¡¬æ ¸æ•¸å€¼ç¨½æ ¸ ---
        # å¾ AI æå–çš„æ•¸æ“šä¸­åŸ·è¡Œ Python åˆ¤å®š
        dim_data = res_main.get("dimension_data", [])
        python_numeric_issues = python_numerical_audit(dim_data)
        
        # --- 3. Python è¡¨é ­æª¢æŸ¥ (åŸæœ‰åŠŸèƒ½) ---
        python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)
        
        # --- 4. åˆä½µçµæœ ---
        ai_raw_issues = res_main.get("issues", [])
        ai_filtered_issues = []

        for i in ai_raw_issues:
            i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
            i_type = i.get("issue_type", "")
            common_reason = i.get("common_reason", "")

            # ğŸ’¡ å…¨æ–¹ä½é˜²è­·ï¼š
            # 1. æ””æˆª AI å°ã€ŒéŠ²è£œã€å°ºå¯¸è®Šå¤§çš„èª¤åˆ¤ (å› ç‚ºéŠ²è£œæœ¬ä¾†å°±æœƒè®Šå¤§)
            if "éŠ²è£œ" in i.get("item", "") and "å¢åŠ å°ºå¯¸" in common_reason:
                continue
            
            # 2. ä¿ç•™æœƒè¨ˆã€æ•¸é‡ã€çµ±è¨ˆã€é‹è²»ã€æµç¨‹ã€ä¾è³´ã€è¡¨é ­
            keep_keywords = ["çµ±è¨ˆ", "æ•¸é‡", "ä¸ç¬¦", "æµç¨‹", "é †åº", "å¹½éˆ", "ä¾è³´", "è¡¨é ­", "é‹è²»", "æœªåŒ¹é…"]
            if any(kw in i_type for kw in keep_keywords):
                ai_filtered_issues.append(i)
                
            # 3. éæ¿¾æ‰ AI çš„ã€Œæ•¸å€¼ã€å°ºå¯¸ã€æ ¼å¼ã€åˆ¤æ–·ï¼Œå› ç‚º Python æ¯”è¼ƒæº–
            elif "æ•¸å€¼" not in i_type and "å°ºå¯¸" not in i_type and "æ ¼å¼" not in i_type:
                ai_filtered_issues.append(i)
            
        all_issues = ai_filtered_issues + python_numeric_issues + python_header_issues
        
        st.session_state.analysis_result_cache = {
            "job_no": res_main.get("job_no", "Unknown"),
            "all_issues": all_issues,
            "total_duration": total_end - total_start,
            "cost_twd": cost_twd,
            "total_in": usage_main["input"],
            "total_out": usage_main["output"],
            "ocr_duration": ocr_duration,
            "time_eng": time_main, # é€™è£¡å€Ÿç”¨è®Šæ•¸åï¼Œå¯¦ç‚ºç¸½æ™‚é–“
            "time_acc": 0,         # å–®ä¸€ä»£ç†ç„¡ç¬¬äºŒæ™‚é–“
            "full_text_for_search": full_text_for_search,
            "combined_input": combined_input,
            "python_debug_data": python_debug_data
        }

    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache['all_issues']
        
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")
        
        with st.expander("ğŸ” æŸ¥çœ‹ AI è®€å–åˆ°çš„ Excel è¦å‰‡ (Debug)"):
            rules_text = get_dynamic_rules(cache['full_text_for_search'], debug_mode=True)
            if "ç„¡ç‰¹å®šè¦å‰‡" in rules_text:
                st.caption("ç„¡åŒ¹é…è¦å‰‡")
            else:
                st.markdown(rules_text)

        with st.expander("ğŸ æŸ¥çœ‹ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ (Debug)", expanded=False):
            if cache.get('python_debug_data'):
                p_data = cache['python_debug_data']
                standard_data = {}
                all_values = {"å·¥ä»¤ç·¨è™Ÿ": [], "é å®šäº¤è²¨": [], "å¯¦éš›äº¤è²¨": []}
                for page in p_data:
                    for k in all_values.keys():
                        if page.get(k) and page[k] != "N/A":
                            all_values[k].append(page[k])
                
                standard_row = {"é ç¢¼": "ğŸ† åˆ¤å®šæ¨™æº–"}
                for k, v in all_values.items():
                    if v:
                        standard_row[k] = Counter(v).most_common(1)[0][0]
                    else:
                        standard_row[k] = "N/A"
                
                final_df_data = [standard_row] + p_data
                st.dataframe(final_df_data, use_container_width=True, hide_index=True)
                st.info("ğŸ’¡ ã€Œåˆ¤å®šæ¨™æº–ã€æ˜¯ä¾æ“šå¤šæ•¸æ±ºç”¢ç”Ÿçš„ã€‚")
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        real_errors = [i for i in all_issues if "æœªåŒ¹é…" not in i.get('issue_type', '')]
        
        if not real_errors:
            st.balloons()
            if not all_issues:
                st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
            else:
                st.success(f"âœ… æ•¸å€¼å…¨æ•¸åˆæ ¼ï¼ (ä½†æœ‰ {len(all_issues)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡ï¼Œè«‹æª¢æŸ¥)")
        else:
            st.error(f"ç™¼ç¾ {len(real_errors)} é¡æ•¸å€¼ç•°å¸¸ï¼Œå¦æœ‰ {len(all_issues) - len(real_errors)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡")

        for item in all_issues:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                
                source_label = item.get('source', '')
                rule_source = item.get('rule_used', 'ç³»çµ±é è¨­é‚è¼¯')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                common_reason = item.get('common_reason', '')
                
                c1.markdown(f"**P.{item.get('page', '?')} | {item.get('item')}**  `{source_label}`")
                
                if "Excel" in rule_source:
                    c1.caption(f"ğŸ“œ åˆ¤æ–·ä¾æ“š: :blue-background[{rule_source}]")
                elif "ç„¡å°æ‡‰" in rule_source or "ç›²æ¸¬" in rule_source:
                    c1.caption(f"âš ï¸ åˆ¤æ–·ä¾æ“š: :grey-background[â“ ç„¡å°æ‡‰è¦å‰‡ (ç›²æ¸¬)]")
                else:
                    c1.caption(f"ğŸ¤– åˆ¤æ–·ä¾æ“š: {rule_source}")
                
                if "æœªåŒ¹é…" in issue_type:
                    if "åˆæ ¼" in common_reason:
                        c2.warning(f"âš ï¸ æœªåŒ¹é…") 
                    else:
                        c2.error(f"ğŸ›‘ æœªåŒ¹é…è¶…è¦") 
                elif "æµç¨‹" in issue_type or "å°ºå¯¸" in issue_type or "çµ±è¨ˆ" in issue_type:
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {common_reason}")
                
                spec = item.get('spec_logic') or item.get('target_spec')
                if spec: st.caption(f"æ¨™æº–: {spec}")
                
                if item.get('verification_logic'): st.caption(f"é©—è­‰: {item.get('verification_logic')}")
                
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            # å°‡ id æ”¹ç‚ºã€Œé …ç›®/ç·¨è™Ÿã€ï¼Œé€™æ¨£æœƒè¨ˆä¾†æºæœƒé¡¯ç¤ºåœ¨ç¬¬ä¸€æ¬„
                            row = {
                                "é …ç›®/ç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A'),
                                "è¦æ ¼/å‚™è¨»": f.get('target', ''),
                                "åˆ¤å®šç®—å¼": f.get('calc', '')
                            }
                            table_data.append(row)
                    
                    if table_data:
                        st.dataframe(table_data, use_container_width=True, hide_index=True)
                
                elif 'roll_id' in item:
                    table_data = [{
                        "æ»¾è¼ªç·¨è™Ÿ": item.get('roll_id'),
                        "å¯¦æ¸¬å€¼": item.get('raw_value'),
                        "è¦æ ¼": item.get('target_spec')
                    }]
                    st.dataframe(table_data, use_container_width=True, hide_index=True)
                else:
                    st.text(f"å¯¦æ¸¬æ•¸æ“š: {item.get('measured', 'N/A')}")
        
        st.divider()

        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache['combined_input'], language='markdown')
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
