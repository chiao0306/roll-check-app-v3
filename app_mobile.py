import streamlit as st
import streamlit.components.v1 as components
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
from openai import OpenAI
import json
import time
import concurrent.futures
import pandas as pd
from thefuzz import fuzz
from collections import Counter
import re

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="äº¤è²¨å–®ç¨½æ ¸(å–®ä¸€ä»£ç†)", page_icon="ğŸ­", layout="centered")

# --- CSS æ¨£å¼ ---
st.markdown("""
<style>
/* 1. æ¨™é¡Œå¤§å°æ§åˆ¶ */
h1 {
    font-size: 1.7rem !important; 
    white-space: nowrap !important;
    overflow: hidden !important; 
    text-overflow: ellipsis !important;
}

/* 2. ä¸»åŠŸèƒ½æŒ‰éˆ• (ç´…è‰² Primary) -> è®Šå¤§ã€è®Šé«˜ */
/* é€™æœƒå½±éŸ¿ã€Œé–‹å§‹åˆ†æã€å’Œã€Œç…§ç‰‡æ¸…é™¤ã€ */
button[kind="primary"] {
    height: 60px;               
    font-size: 20px !important; 
    font-weight: bold !important;
    border-radius: 10px !important;
    margin-top: 0px !important;    
    margin-bottom: 5px !important; 
    width: 100%;                
}

/* 3. æ¬¡è¦æŒ‰éˆ• (ç°è‰² Secondary) -> ä¿æŒåŸç‹€ */
/* é€™æœƒå½±éŸ¿æ¯ä¸€å¼µç…§ç‰‡ä¸‹é¢çš„ã€ŒXã€æŒ‰éˆ•ï¼Œè®“å®ƒç¶­æŒå°å°çš„ */
button[kind="secondary"] {
    height: auto !important;
    font-weight: normal !important;
}
</style>
""", unsafe_allow_html=True)
# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
    OPENAI_KEY = st.secrets.get("OPENAI_KEY", "")
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state: st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0
if 'auto_start_analysis' not in st.session_state: st.session_state.auto_start_analysis = False

# --- å´é‚Šæ¬„æ¨¡å‹è¨­å®š (åˆä½µç‚ºå–®ä¸€é¸æ“‡) ---
with st.sidebar:
    st.header("æ¨¡å‹è¨­å®š")
    
    # é€™è£¡åŠ å…¥æœ€æ–°çš„ Gemini æ¨¡å‹
    model_options = {
        "Gemini 3 Flash preview": "gemini-3-pro-image-preview",
        "Gemini 2.5 Flash": "models/gemini-2.5-flash",
        "Gemini 2.5 Pro": "models/gemini-2.5-pro",
        #"GPT-5(ç„¡æ•ˆ)": "models/gpt-5",
        #"GPT-5 Mini(ç„¡æ•ˆ)": "models/gpt-5-mini",
    }
    options_list = list(model_options.keys())
    
    st.subheader("ğŸ¤– ç¸½ç¨½æ ¸ Agent")
    model_selection = st.selectbox(
        "è² è²¬ï¼šè¦æ ¼ã€è£½ç¨‹ã€æ•¸é‡ã€çµ±è¨ˆå…¨åŒ…", 
        options=options_list, 
        index=0, 
        key="main_model"
    )
    main_model_name = model_options[model_selection]
    
    st.divider()
    
    default_auto = st.query_params.get("auto", "true") == "true"
    def update_url_param():
        current_state = "true" if st.session_state.enable_auto_analysis else "false"
        st.query_params["auto"] = current_state

    st.toggle(
        "âš¡ ä¸Šå‚³å¾Œè‡ªå‹•åˆ†æ", 
        value=default_auto, 
        key="enable_auto_analysis", 
        on_change=update_url_param
    )

# --- Excel è¦å‰‡è®€å–å‡½æ•¸ (å–®ä¸€ä»£ç†æ•´åˆç‰ˆ) ---
@st.cache_data
def get_dynamic_rules(ocr_text, debug_mode=False):
    try:
        df = pd.read_excel("rules.xlsx")
        df.columns = [c.strip() for c in df.columns]
        
        ocr_text_clean = str(ocr_text).upper().replace(" ", "").replace("\n", "")
        
        specific_rules = []
        general_rules = []
        match_log = []

        for index, row in df.iterrows():
            item_name = str(row.get('Item_Name', '')).strip()
            
            # --- è®€å–å·¥ç¨‹æ¬„ä½ ---
            spec = str(row.get('Standard_Spec', ''))
            if str(spec).lower() == 'nan': spec = ""
            
            category = str(row.get('Category', ''))
            if str(category).lower() == 'nan': category = ""
            
            logic = str(row.get('Logic_Prompt', ''))
            if str(logic).lower() == 'nan': logic = ""
            
            # --- è®€å–æœƒè¨ˆä¸‰æ¬„ä½ (æ–°åŠŸèƒ½) ---
            # 1. å–®é …æ ¸å°
            u_local = str(row.get('Unit_Rule_Local', ''))
            if u_local.lower() == 'nan': u_local = ""
            
            # 2. èšåˆçµ±è¨ˆ
            u_agg = str(row.get('Unit_Rule_Agg', ''))
            if u_agg.lower() == 'nan': u_agg = ""
            
            # 3. é‹è²»è¨ˆç®—
            u_freight = str(row.get('Unit_Rule_Freight', ''))
            if u_freight.lower() == 'nan': u_freight = ""
            
            keywords = str(row.get('Trigger_Keywords', ''))
            if str(keywords).lower() == 'nan': keywords = ""
            
            is_general_rule = "(é€šç”¨)" in item_name
            
            # --- æƒ…å¢ƒ A: é€šç”¨è¦å‰‡ ---
            if is_general_rule:
                if not keywords:
                    rule_desc = f"- **[å…¨åŸŸæ†²æ³•] {item_name}**\n  - æŒ‡ä»¤: {logic}"
                    general_rules.append(rule_desc)
                    if debug_mode: match_log.append(f"âš–ï¸ [æ†²æ³•] {item_name} (å¼·åˆ¶è¼‰å…¥)")
                
                elif keywords:
                    cleaned_kws = keywords.replace("ï¼Œ", ",").replace("ã€", ",").split(",")
                    cleaned_kws = [k.strip() for k in cleaned_kws if k.strip()]
                    formatted_keywords = str(cleaned_kws)

                    rule_desc = (
                        f"- **{item_name}**\n"
                        f"  - è§¸ç™¼é—œéµå­—: `{formatted_keywords}`\n"
                        f"  - é‚è¼¯: {logic}"
                    )
                    general_rules.append(rule_desc)
                    if debug_mode: match_log.append(f"ğŸ“š [é€šç”¨] {item_name} (é—œéµå­—: {formatted_keywords})")
            
            # --- æƒ…å¢ƒ B: ç‰¹å®šå°ˆæ¡ˆè¦å‰‡ ---
            else:
                if not item_name: continue
                keyword_clean = item_name.upper().replace(" ", "")
                
                score = fuzz.partial_ratio(keyword_clean, ocr_text_clean)
                threshold = 85
                
                if debug_mode:
                    status_icon = "âœ…" if score >= threshold else "âŒ"
                    match_log.append(f"- {status_icon} **[ç‰¹è¦] {item_name}** | åˆ†æ•¸: `{score}`")
                
                if score >= threshold:
                    desc = f"- **[ç‰¹å®š] {item_name}**"
                    # å·¥ç¨‹è³‡è¨Š
                    if spec: desc += f"\n  - [å·¥]è¦æ ¼æ¨™æº–: {spec}"
                    if logic: desc += f"\n  - [å·¥]ç‰¹æ®ŠæŒ‡ä»¤: {logic}"
                    if category: desc += f"\n  - [å·¥]åˆ†é¡: {category}"
                    
                    # æœƒè¨ˆè³‡è¨Š (åˆ†é–‹åˆ—å‡ºï¼Œè®“ AI å°è™Ÿå…¥åº§)
                    if u_local:   desc += f"\n  - [æœƒ]å–®é …æ ¸å°è¦å‰‡: **{u_local}**"
                    if u_agg:     desc += f"\n  - [æœƒ]èšåˆçµ±è¨ˆè¦å‰‡: **{u_agg}**"
                    if u_freight: desc += f"\n  - [æœƒ]é‹è²»è¨ˆç®—è¦å‰‡: **{u_freight}**"
                    
                    specific_rules.append(desc)
        
        final_output = ""
        
        if specific_rules:
            final_output += "### ğŸ¯ ç¬¬ä¸€å€ï¼šå°ˆæ¡ˆç‰¹å®šè¦å‰‡ (æœ€é«˜æ¬Šé™)\n" + "\n".join(specific_rules) + "\n\n"
            
        if general_rules:
            final_output += "### ğŸ“š ç¬¬äºŒå€ï¼šé€šç”¨é‚è¼¯è³‡æ–™åº« (åŸºç¤é‚è¼¯)\n"
            final_output += "\n".join(general_rules)
            
        if not final_output:
            final_output = "ç„¡ç‰¹å®šè¦å‰‡ã€‚"

        if debug_mode:
            final_output += "\n\n---\n### ğŸ•µï¸â€â™‚ï¸ è¦å‰‡åŒ¹é…æ—¥èªŒ (Match Log)\n" + "\n".join(match_log)
            
        return final_output

    except Exception as e:
        return f"è®€å–è¦å‰‡æª”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"

# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ ---
def extract_layout_with_azure(file_obj, endpoint, key):
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    file_content = file_obj.getvalue()
    
    poller = client.begin_analyze_document("prebuilt-layout", file_content, content_type="application/octet-stream")
    result: AnalyzeResult = poller.result()
    
    markdown_output = ""
    full_content_text = ""
    real_page_num = "Unknown"
    
    bottom_stop_keywords = ["æ³¨æ„äº‹é …", "ä¸­æ©Ÿå“æª¢å–®ä½", "ä¿å­˜æœŸé™", "è¡¨å–®ç·¨è™Ÿ", "FORM NO", "ç°½ç« "]
    top_right_noise_keywords = [
        "æª¢é©—é¡åˆ¥", "å°ºå¯¸æª¢é©—", "ä¾åœ–é¢æ¨™è¨˜", "ææ–™æª¢é©—", "æˆä»½åˆ†æ", 
        "éç ´å£æ€§", "æ­£å¸¸åŒ–", "é€€ç«", "æ·¬.å›ç«", "è¡¨é¢ç¡¬åŒ–", "è©¦è»Š",
        "æ€§èƒ½æ¸¬è©¦", "è©¦å£“è©¦æ¼", "å‹•.éœå¹³è¡¡è©¦é©—", ":selected:", ":unselected:",
        "æŠ—æ‹‰", "ç¡¬åº¦è©¦é©—", "UT", "PT", "MT"
    ]
    
    if result.tables:
        for idx, table in enumerate(result.tables):
            page_num = "Unknown"
            if table.bounding_regions: page_num = table.bounding_regions[0].page_number
            markdown_output += f"\n### Table {idx + 1} (Page {page_num}):\n"
            rows = {}
            stop_processing_table = False 
            
            for cell in table.cells:
                if stop_processing_table: break
                content = cell.content.replace("\n", " ").strip()
                
                for kw in bottom_stop_keywords:
                    if kw in content:
                        stop_processing_table = True
                        break
                if stop_processing_table: break
                
                is_noise = False
                for kw in top_right_noise_keywords:
                    if kw in content:
                        is_noise = True
                        break
                if is_noise: content = "" 

                r, c = cell.row_index, cell.column_index
                if r not in rows: rows[r] = {}
                rows[r][c] = content
            
            for r in sorted(rows.keys()):
                row_cells = []
                if rows[r]:
                    max_col = max(rows[r].keys())
                    for c in range(max_col + 1): 
                        row_cells.append(rows[r].get(c, ""))
                    markdown_output += "| " + " | ".join(row_cells) + " |\n"
    
    if result.content:
        match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", result.content, re.IGNORECASE)
        if match:
            real_page_num = match.group(1)

        cut_index = len(result.content)
        for keyword in bottom_stop_keywords:
            idx = result.content.find(keyword)
            if idx != -1 and idx < cut_index:
                cut_index = idx
        
        temp_text = result.content[:cut_index]
        for noise in top_right_noise_keywords:
            temp_text = temp_text.replace(noise, "")
            
        full_content_text = temp_text
        header_snippet = full_content_text[:800]
    else:
        full_content_text = ""
        header_snippet = ""

    return markdown_output, header_snippet, full_content_text, result.as_dict(), real_page_num

# --- Python ç¡¬é‚è¼¯ï¼šè¡¨é ­ä¸€è‡´æ€§æª¢æŸ¥ (é•·åº¦æ•æ„Ÿç‰ˆ) ---
def python_header_check(photo_gallery):
    issues = []
    if not photo_gallery:
        return issues, []

    # å®šç¾© Regex (é‡å° "å»ç©ºç™½+å»æ›è¡Œ" å¾Œçš„å­—ä¸²è¨­è¨ˆ)
    patterns = {
        # ã€ä¿®æ”¹é» 1ã€‘å·¥ä»¤ Regex æ”¾å¯¬ï¼š
        # åŸæœ¬åªæŠ“ W é–‹é ­ï¼Œç¾åœ¨æ”¹æŠ“ "ç·¨è™Ÿ" å¾Œé¢æ¥çš„ "ä»»ä½•è‹±æ•¸å­—ä¸²"
        # é€™æ¨£å°±ç®—å®ƒå¯« WW363... æˆ–æ˜¯ 12345... éƒ½èƒ½æ•´ä¸²æŠ“å‡ºä¾†æ¯”å°
        "å·¥ä»¤ç·¨è™Ÿ": r"[å·¥åœŸä¸‹][ä»¤å†·ä»Š]ç·¨è™Ÿ[:\.]*([A-Za-z0-9\-\_]+)", 
        
        "é å®šäº¤è²¨": r"[é é¢„é …é ‚][å®šäº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})",
        "å¯¦éš›äº¤è²¨": r"[å¯¦çœŸ][éš›äº¤].*?(\d{2,4}[\.\-/]\d{1,2}[\.\-/]\d{1,2})"
    }

    extracted_data = [] 
    all_values = {key: [] for key in patterns}

    for i, page in enumerate(photo_gallery):
        # æš´åŠ›æ¸…æ´—ï¼šå»æ›è¡Œã€å»ç©ºæ ¼ã€è½‰å¤§å¯«
        raw_text = page.get('header_text', '') + page.get('full_text', '')
        clean_text = raw_text.replace("\n", "").replace(" ", "").replace("\r", "").upper()
        
        # ã€ä¿®æ”¹é» 2ã€‘é ç¢¼é˜²å‘†ï¼šç¢ºä¿ä¸€å®šæœ‰å€¼
        # å„ªå…ˆæŠ“ real_pageï¼ŒæŠ“ä¸åˆ°å°±ç”¨ index
        r_page = page.get('real_page')
        if not r_page or r_page == "Unknown":
            page_label = f"P.{i + 1}"
        else:
            page_label = f"P.{r_page}"
            
        page_result = {"é æ•¸": page_label}
        
        for key, pattern in patterns.items():
            match = re.search(pattern, clean_text)
            if match:
                val = match.group(1).strip()
                
                # ã€ä¿®æ”¹é» 3ã€‘é‡å°å·¥ä»¤çš„ç‰¹æ®Šè™•ç† (å¦‚æœå¤ªé•·å¯èƒ½å°±æ˜¯é‡è¤‡æ‰“å­—)
                if key == "å·¥ä»¤ç·¨è™Ÿ":
                    # å¦‚æœä½ ç¢ºå®šå·¥ä»¤åªæœ‰ 10 ç¢¼ï¼Œä½†æŠ“åˆ°äº† 11 ç¢¼ä»¥ä¸Š (å¦‚ WW...)
                    # æˆ‘å€‘ä¿ç•™é€™å€‹éŒ¯èª¤çš„å€¼ï¼Œè®“å¾Œé¢çš„å¤šæ•¸æ±ºå»æŠŠå®ƒæªå‡ºä¾†
                    pass 
                
                page_result[key] = val
                all_values[key].append(val)
            else:
                page_result[key] = "N/A"
        
        extracted_data.append(page_result)

    # æ­¥é©Ÿ 2: æ±ºå®šã€Œæ­£ç¢ºæ¨™æº–ã€ (ä½¿ç”¨å¤šæ•¸æ±º)
    standard_data = {}
    for key, values in all_values.items():
        if values:
            # æ¿¾æ‰ N/A å¾Œå†æŠ•ç¥¨
            valid_values = [v for v in values if v != "N/A"]
            if valid_values:
                most_common = Counter(valid_values).most_common(1)[0][0]
                standard_data[key] = most_common
            else:
                standard_data[key] = "N/A"
        else:
            standard_data[key] = "N/A"

    # æ­¥é©Ÿ 3: æ¯”å°æ¯ä¸€é 
    for data in extracted_data:
        page_num = data['é æ•¸']
        
        for key, standard_val in standard_data.items():
            current_val = data[key]
            
            if standard_val == "N/A": continue # å…¨å·éƒ½æ²’æŠ“åˆ°å°±ä¸æ¯”äº†

            # é–‹å§‹æ¯”å° (å­—ä¸²ä¸ç›¸ç­‰)
            if current_val != standard_val:
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºé•·åº¦ç•°å¸¸ (é‡å°å·¥ä»¤)
                reason = "èˆ‡å…¨å·å¤šæ•¸é é¢ä¸ä¸€è‡´"
                if key == "å·¥ä»¤ç·¨è™Ÿ" and len(current_val) != len(standard_val):
                    reason += f" (é•·åº¦ç•°å¸¸: {len(current_val)}ç¢¼ vs æ¨™æº–{len(standard_val)}ç¢¼)"

                issue = {
                    "page": page_num.replace("P.", ""),
                    "item": f"è¡¨é ­æª¢æŸ¥-{key}",
                    "rule_used": "Pythonç¡¬é‚è¼¯æª¢æŸ¥",
                    "issue_type": "è·¨é è³‡è¨Šä¸ç¬¦",
                    "spec_logic": f"æ‡‰ç‚º {standard_val}",
                    "common_reason": reason,
                    "failures": [
                        {"id": "å…¨å·åŸºæº–", "val": standard_val, "calc": "å¤šæ•¸æ±ºæ¨™æº–"},
                        {"id": f"æœ¬é ({page_num})", "val": current_val, "calc": "ç•°å¸¸/æ¼æŠ“"}
                    ],
                    "source": "ğŸ¤– ç³»çµ±è‡ªå‹•"
                }
                issues.append(issue)
                
    return issues, extracted_data

# --- 5. ç¸½ç¨½æ ¸ Agent (æ•´åˆç‰ˆ - å¼·é‚è¼¯å„ªåŒ–) ---
def agent_unified_check(combined_input, full_text_for_search, api_key, model_name):
    
    # è®€å–æ‰€æœ‰è¦å‰‡
    dynamic_rules = get_dynamic_rules(full_text_for_search)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ã€ç¸½ç¨½æ ¸å®˜ã€‘ã€‚
    ä½ çš„å¤§è…¦é‹ä½œå¿…é ˆåƒã€Œé›»è…¦ç¨‹å¼ã€ä¸€æ¨£ï¼Œåš´æ ¼éµå®ˆä»¥ä¸‹çš„ã€Œæ³•å¾‹éšç´šã€èˆ‡ã€ŒåŸ·è¡Œæµç¨‹ã€ã€‚
    å®Œå…¨ä¾ç…§è¦å‰‡ï¼Œç¦æ­¢è‡ªå·±è§£é‡‹ã€‚

    ### ğŸ§  ä½ çš„çŸ¥è­˜åº« (Knowledge Base)
    {dynamic_rules}
    
    ---

    ### âš–ï¸ åˆ¤æ±ºæ†²æ³• (Hierarchy of Authority)
    **è«‹æ³¨æ„ï¼šåˆ¤å®šæ¨™æº–åˆ†ç‚ºã€Œæ•¸æ“šå±¤ã€èˆ‡ã€Œé‚è¼¯å±¤ã€ï¼Œå…©è€…å¿…é ˆåŒæ™‚æˆç«‹ã€‚**

    **ç¬¬ 1 éšç´šï¼š[ç¬¬ä¸€å€ï¼šå°ˆæ¡ˆç‰¹å®šè¦å‰‡] (Specific Data)**
    *   **æ¬ŠåŠ›**ï¼šå®šç¾©è©²é …ç›®çš„ **ã€Œç›®æ¨™æ•¸å€¼ã€**ã€‚è‹¥æœ‰æ•¸å€¼ï¼Œä»¥æ­¤ç‚ºæº–ã€‚
    *   **æŒ‡ä»¤**ï¼šè‹¥ `ç‰¹æ®ŠæŒ‡ä»¤(Logic)` ç‚ºç©ºï¼Œä»£è¡¨ **ã€Œå®Œå…¨éµå®ˆé€šç”¨é‚è¼¯ã€**ã€‚

    **ç¬¬ 2 éšç´šï¼š[ç¬¬äºŒå€ï¼šé€šç”¨é‚è¼¯è³‡æ–™åº«] (General Logic)**
    *   **æ¬ŠåŠ›**ï¼šå®šç¾©å…¨å» é€šç”¨çš„ **ã€Œç‰©ç†æ³•å‰‡ã€** (å¦‚é †åºã€ä¾è³´æ€§)ã€‚
    *   **å¼·åˆ¶æ€§**ï¼š**é è¨­ç‚ºé–‹å•Ÿç‹€æ…‹**ã€‚é™¤éç¬¬ 1 éšç´šæ˜ç¢ºå¯«å‡ºã€Œè±å…ã€ï¼Œå¦å‰‡ä¸å¯é—œé–‰ã€‚

    ---

    ### ğŸš€ åŸ·è¡Œç¨‹åº (Execution Procedure)

    #### âš”ï¸ æ¨¡çµ„ Aï¼šå·¥ç¨‹è¦æ ¼ç¨½æ ¸ (Engineering)
    **åˆ¤å®šå…¬å¼ï¼šPASS = (Step 1 æ•¸å€¼åˆæ ¼) AND (Step 2 é‚è¼¯åˆæ ¼)**

    **Step 1: ç‰¹è¦æŒ‡ä»¤èˆ‡æ•¸å€¼æª¢æŸ¥**
    *   **è®€å–**ï¼š[ç¬¬ä¸€å€] çš„ `Standard_Spec` èˆ‡ `Logic_Prompt`ã€‚
    *   **æª¢æŸ¥æŒ‡ä»¤**ï¼šè‹¥ `Logic_Prompt` æœ‰å…§å®¹ï¼Œå„ªå…ˆåŸ·è¡Œã€‚
    *   **æ¯”å°æ•¸å€¼**ï¼šè‹¥æœ‰ `Standard_Spec`ï¼Œä»¥æ­¤ç‚ºæ¨™æº–ã€‚

    **Step 2: ç‰©ç†èˆ‡é€šç”¨é‚è¼¯æª¢æŸ¥ (CRITICAL STEP)**
    *   **å•Ÿå‹•æ¢ä»¶**ï¼š
        *   `IF` Step 1 çš„ `Logic_Prompt` æ˜¯ **ç©ºç™½ (Empty)** -> **å¿…é ˆåŸ·è¡Œ Step 2**ã€‚
        *   `IF` Step 1 çš„ `Logic_Prompt` å¯«äº† "è±å…" -> åªæœ‰é€™ç¨®æƒ…æ³æ‰å¯è·³éã€‚
    *   **åŸ·è¡Œå‹•ä½œ**ï¼š
        1.  **ç‰©ç†é †åº**ï¼šæª¢æŸ¥ `æœªå†ç”Ÿ(å°) < ç ”ç£¨(ä¸­) < å†ç”Ÿ(å¤§) < éŠ²è£œ(æœ€å¤§)`ã€‚è‹¥é•å -> **FAIL**ã€‚
        2.  **ä¾è³´æ€§**ï¼šæª¢æŸ¥å‰å¾Œè£½ç¨‹æ˜¯å¦å­˜åœ¨ã€‚
        3.  **é€šç”¨æ ¼å¼**ï¼šè‹¥é€šç”¨è¦å‰‡è¦æ±‚å…©ä½å°æ•¸ï¼Œå¯¦æ¸¬å€¼é ˆç¬¦åˆã€‚

    ### ğŸš€ åŸ·è¡Œæ¨¡çµ„ Bï¼šæœƒè¨ˆæ•¸é‡æ ¸å° (ä¸‰éšæ®µç¨ç«‹åƒæ•¸)
    **è«‹æ³¨æ„ï¼šæœƒè¨ˆæª¢æŸ¥åˆ†ç‚ºä¸‰å€‹ç¨ç«‹æ­¥é©Ÿï¼Œæ¯å€‹æ­¥é©Ÿå¿…é ˆåƒè€ƒ Excel å°æ‡‰çš„è¦å‰‡æ¬„ä½ã€‚**
    
    **Step 1: å–®é …æ•¸é‡è¨ˆç®— (Local Calculation)**
    *   **ç®—æ³•**ï¼šé …ç›®è¨ˆæ•¸ï¼ˆç›®æ¨™æ•¸ï¼‰ = åˆ—è¡¨çš„"ç·¨è™Ÿ"å€‹æ•¸ã€‚
        ä¾‹ï¼šè¦ç¯„æ¨™æº–ï¼šW3 #6 295ï¼ˆXï¼‰ ROLL æœ¬é«”æœªå†ç”Ÿè»Šä¿®ï¼ˆ12PCï¼‰ï¼Œæ­¤é …ç›®å¾Œè¦æœ‰12å€‹ç·¨è™Ÿã€‚
        *   **æœ¬é«” (Body)**: ä½¿ç”¨ `Count Distinct` (å»é‡è¨ˆç®—ç¨ç«‹ç·¨è™Ÿ)ã€‚
        *   **è»¸é ¸/å…§å­”**: ä½¿ç”¨ `Count Total Rows` (è¨ˆç®—ç¸½è¡Œæ•¸)ï¼Œé …ç›®å…§æ¯å€‹ç¨ç«‹ç·¨è™Ÿ**ä¸å¯é‡è¤‡è¶…é2æ¬¡**ã€‚
        *   **åƒæ•¸ä¾†æº**ï¼šæŸ¥çœ‹ç‰¹è¦çš„ **`[æœƒ]å–®é …æ ¸å°è¦å‰‡`**ã€‚
        *   è‹¥æœ‰ (å¦‚ "1SET=4PCS")ï¼šä»¥æ­¤ç‚ºæº–è¨ˆç®— (Rows / 4)ã€‚
        *   è‹¥ç„¡ï¼šé è¨­ `1 SET = 2 PCS`, `1 PC = 1 PC`ã€‚
      
    **Step 2: ç¸½è¡¨æ ¸å° (Global Summary Check)**
    *   **ç›®æ¨™**ï¼šæ ¸å°å·¦ä¸Šè§’ã€Œå¯¦äº¤æ•¸é‡ã€ vs ã€Œè·¨é å…§æ–‡é …ç›®åŠ ç¸½ã€ã€‚
    *   **åŸ·è¡Œé‚è¼¯**ï¼šè«‹å…ˆè®€å–å·¦ä¸Šè§’çš„ã€Œé …ç›®åç¨±ã€ï¼Œä¾æ“šä¸‹åˆ—è¦å‰‡æ±ºå®šå“ªäº›ã€Œå…§æ–‡é …ç›®ã€éœ€è¦è¢«åŠ ç¸½ï¼š

    **A. é›™è»Œèšåˆæ¨¡å¼ (Aggregated Mode)**
    *   **è§¸ç™¼æ¢ä»¶**ï¼šç•¶å·¦ä¸Šè§’é …ç›®åç¨± **åŒæ™‚åŒ…å«** ã€ŒROLLã€ èˆ‡ ã€Œè»Šä¿® / éŠ²è£œ / æ‹†è£ã€å…¶ä¸­ä¹‹ä¸€æ™‚ã€‚
        *   *(ä¾‹å¦‚ï¼š"W3 #1æ©Ÿ ROLL è»Šä¿®", "ROLL éŠ²è£œ")*
    *   **åŠ ç¸½ç¯„åœ (é è¨­)**ï¼š
        *   **è»Šä¿®** = Sum(æœ¬é«”æœªå†ç”Ÿ + æœ¬é«”å†ç”Ÿ + è»¸é ¸æœªå†ç”Ÿ + è»¸é ¸å†ç”Ÿ)
        *   **éŠ²è£œ** = Sum(æœ¬é«”éŠ²è£œ + è»¸é ¸éŠ²è£œ)
        *   **æ‹†è£** = Sum(æ–°å“çµ„è£ + èˆŠå“æ‹†è£)
    *   **ä¾‹å¤–éæ¿¾ (ç‰¹è¦ä»‹å…¥)**ï¼š
        *   åœ¨åŠ ç¸½ä¸Šè¿°é …ç›®ä¹‹å‰ï¼Œ**å¿…é ˆ**æª¢æŸ¥è©²é …ç›®çš„ **`[æœƒ]èšåˆçµ±è¨ˆè¦å‰‡`**ã€‚
        *   è‹¥å¯« **"è±å…"** æˆ– **"å¼·åˆ¶æ­¸é¡ç‚ºé€šç”¨"**ï¼šâŒ **åš´ç¦**å°‡å…¶åŠ å…¥ä¸Šè¿°ç¸½å¸³ã€‚
        *   è‹¥å¯« **"1SET=1PC"**ï¼šâš ï¸ åƒ…åŠ å…¥ **1** å€‹å–®ä½ (è€Œéå…§æ–‡çš„å¯¦éš›è¡Œæ•¸)ã€‚

    **B. æ¨™æº–å°æ‡‰æ¨¡å¼ (Standard Mode)**
    *   **è§¸ç™¼æ¢ä»¶**ï¼šç•¶å·¦ä¸Šè§’é …ç›®åç¨± **ä¸åŒ…å«** ä¸Šè¿°èšåˆé—œéµå­—æ™‚ (ä¾‹å¦‚ã€Œç†±è™•ç†ã€ã€ã€ŒKeywayã€)ã€‚
    *   **åŠ ç¸½ç¯„åœ**ï¼šåƒ…åŠ ç¸½å…§æ–‡ä¸­ **ã€Œåç¨±å®Œå…¨å°æ‡‰ã€** æˆ– **ã€Œé‚è¼¯ä¸Šå±¬æ–¼è©²é …ç›®ã€** çš„å­é …ç›®ã€‚
    *   **é‚è¼¯**ï¼šæ­¤æ¨¡å¼ä¸‹ï¼Œ**å¿½ç•¥** Excel çš„ `[æœƒ]èšåˆçµ±è¨ˆè¦å‰‡`ã€‚åªè¦åç¨±å°ä¸Šï¼Œå°±ç›´æ¥åŠ ç¸½ã€‚

    **Step 3: é‹è²»è¨ˆç®— (Freight Check)
    *   **ä»»å‹™**ï¼šè¨ˆç®—å…¨å·ã€Œæœ¬é«”æœªå†ç”Ÿè»Šä¿®ã€ç¸½æ•¸ï¼Œæ ¸å°å·¦ä¸Šè§’é‹è²»é …æ¬¡ç¸½æ•¸ã€‚
    *   **åƒæ•¸ä¾†æº**ï¼šæŸ¥çœ‹ç‰¹è¦çš„ **`[æœƒ]é‹è²»è¨ˆç®—è¦å‰‡`**ã€‚
        *   è‹¥å¯« **"è±å…"**ï¼š**åš´ç¦**å°‡æ­¤é …ç›®è¨ˆå…¥é‹è²»ã€‚
        *   è‹¥å¯« **"1SET=1PC"**ï¼šä»¥ 1:1 ç´¯åŠ è‡³é‹è²»ã€‚
        *   è‹¥ç„¡ï¼šé è¨­ä¾æ“š Step 1 çš„çµæœç´¯åŠ ã€‚

    ---
    
    ### ğŸ“ è¼¸å‡ºè¦ç¯„ (Output Format)
    **è«‹å›å‚³å–®ä¸€ JSON ç‰©ä»¶ã€‚**
    
    **ã€é—œéµæŒ‡ä»¤ï¼šçµ±è¨ˆä¸ç¬¦æ™‚çš„å¼·åˆ¶æ ¼å¼ã€‘**
    è‹¥ç™¼ç”Ÿ **çµ±è¨ˆè¡¨æ ¼æ•¸é‡ä¸ç¬¦**ï¼Œ`failures` åˆ—è¡¨ **åš´ç¦** å¯« "Unknown"ã€‚
    **å¿…é ˆ** ç”Ÿæˆä»¥ä¸‹å…©è¡Œå°ç…§æ•¸æ“šï¼š
    1. `{{ "id": "å…§æ–‡é …ç›®åŠ ç¸½", "val": "è¨ˆç®—å€¼", "calc": "è¨ˆç®—" }}`
    2. `{{ "id": "çµ±è¨ˆè¡¨å¯¦äº¤æ•¸é‡", "val": "ç›®æ¨™å€¼", "calc": "ç›®æ¨™" }}`

    {{
      "job_no": "å·¥ä»¤ç·¨è™Ÿ",
      "issues": [
         {{
           "page": "é ç¢¼",
           "item": "é …ç›®åç¨±",
           "rule_used": "ä¾æ“šçš„è¦å‰‡ (è«‹è¨»æ˜æ˜¯ ç‰¹è¦ é‚„æ˜¯ é€šç”¨)",
           "issue_type": "æ•¸å€¼è¶…è¦ / æµç¨‹ç•°å¸¸ / æ•¸é‡ä¸ç¬¦ / çµ±è¨ˆä¸ç¬¦",
           "spec_logic": "åˆ¤å®šæ¨™æº–",
           "common_reason": "ç°¡çŸ­åŸå›  (é™15å­—)",
           "failures": [
              {{ "id": "æ»¾è¼ªç·¨è™Ÿ/é …ç›®", "val": "å¯¦æ¸¬å€¼/è¨ˆæ•¸", "target": "è¦æ ¼/å‚™è¨»", "calc": "å·®å€¼(è‹¥æœ‰)" }}
           ]
         }}
      ]
    }}
    """
    
    generation_config = {"response_mime_type": "application/json", "temperature": 0.0, "top_k": 1, "top_p": 0.95}
    
    try:
        # === åˆ†æµ A: Google Gemini ===
        if "gemini" in model_name.lower():
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model_name)
            response = model.generate_content([system_prompt, combined_input], generation_config=generation_config)
            
            raw_content = response.text
            usage_meta = response.usage_metadata
            usage_in = usage_meta.prompt_token_count if usage_meta else 0
            usage_out = usage_meta.candidates_token_count if usage_meta else 0

        # === åˆ†æµ B: OpenAI GPT ===
        else:
            if not OPENAI_KEY:
                return {"job_no": "Error", "issues": [{"item": "Error", "common_reason": "ç¼ºå°‘ OpenAI Key"}], "_token_usage": {"input":0, "output":0}}
            
            client = OpenAI(api_key=OPENAI_KEY)
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": combined_input}
                ],
                temperature=0.0
            )
            raw_content = response.choices[0].message.content
            usage_in = response.usage.prompt_tokens
            usage_out = response.usage.completion_tokens

        # =========================================================
        # ğŸ›¡ï¸ çµ•å°é˜²ç¦¦ï¼šJSON è§£æèˆ‡çµæ§‹é‡å»º
        # =========================================================
        
        # 1. æ¸…æ´— Markdown
        if "```json" in raw_content:
            raw_content = raw_content.replace("```json", "").replace("```", "")
        elif "```" in raw_content:
            raw_content = raw_content.replace("```", "")
            
        # 2. å˜—è©¦è§£æ
        try:
            parsed_data = json.loads(raw_content)
        except:
            parsed_data = {"job_no": "JSON Error", "issues": []}

        # 3. å»ºæ§‹æœ€çµ‚å›å‚³ç‰©ä»¶
        final_response = {}

        if isinstance(parsed_data, dict):
            final_response = parsed_data
        elif isinstance(parsed_data, list):
            final_response = {"job_no": "Unknown", "issues": parsed_data}
        else:
            final_response = {"job_no": "Unknown", "issues": []}

        # 4. è£œå…¨å¿…è¦æ¬„ä½
        if "issues" not in final_response:
            final_response["issues"] = []
        if "job_no" not in final_response:
            final_response["job_no"] = "Unknown"

        # 5. ã€ä¿®æ”¹é»ã€‘åƒåœ¾éæ¿¾å™¨ (Garbage Collector) & çŸ›ç›¾æ¸…æ´—
        valid_issues = []
        for i in final_response["issues"]:
            if isinstance(i, dict):
                item_name = i.get("item", "")
                reason = i.get("common_reason", "")
                i_type = i.get("issue_type", "")

                # 1. åŸºæœ¬é˜²å‘†ï¼šæ²’æœ‰ item åç¨±å°±è¸¢æ‰
                if not item_name: 
                    continue

                # 2. ã€é—œéµä¿®æ­£ã€‘çŸ›ç›¾æ¸…æ´—
                # å¦‚æœ AI èªªã€Œåˆæ ¼ã€ï¼Œä½†é€™åˆä¸æ˜¯ã€ŒæœªåŒ¹é…è¦å‰‡ã€çš„å¼·åˆ¶å›å ± -> ä»£è¡¨é€™æ˜¯ AI å¤šå˜´ï¼Œè¸¢æ‰ï¼
                if "åˆæ ¼" in reason and "æœªåŒ¹é…" not in i_type:
                    continue
                
                # 3. å¦‚æœ AI èªªã€Œåˆæ ¼ã€ï¼Œä¸”æ˜¯ã€ŒæœªåŒ¹é…ã€ï¼Œä½† issue_type å»å¯«ã€Œæ•¸å€¼è¶…è¦ã€ -> å¼·åˆ¶ä¿®æ­£é¡å‹
                if "åˆæ ¼" in reason and "æœªåŒ¹é…" in i_type:
                    i["issue_type"] = "âš ï¸æœªåŒ¹é…è¦å‰‡" # å¼·åˆ¶ä¿®æ­£ç‚ºé»ƒè‰²è­¦å‘Š

                valid_issues.append(i)
        
        # å°‡æ¸…æ´—å¾Œçš„ä¹¾æ·¨æ¸…å–®æ”¾å›å»
        final_response["issues"] = valid_issues

        # 6. æ³¨å…¥ Token ç”¨é‡
        final_response["_token_usage"] = {"input": usage_in, "output": usage_out}
        
        return final_response

    except Exception as e:
        return {"job_no": "Error", "issues": [{"item": "System Error", "common_reason": str(e)}], "_token_usage": {"input": 0, "output": 0}}

# --- 6. æ‰‹æ©Ÿç‰ˆ UI èˆ‡ æ ¸å¿ƒåŸ·è¡Œé‚è¼¯ ---
st.title("ğŸ­ äº¤è²¨å–®ç¨½æ ¸(å–®ä¸€ä»£ç†)")

data_source = st.radio(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æºï¼š", 
    ["ğŸ“¸ ä¸Šå‚³ç…§ç‰‡", "ğŸ“‚ ä¸Šå‚³ JSON æª”"], 
    horizontal=True
)

with st.container(border=True):
    if data_source == "ğŸ“¸ ä¸Šå‚³ç…§ç‰‡":
        if st.session_state.get('source_mode') == 'json':
            st.session_state.photo_gallery = []
            st.session_state.source_mode = 'image'

        uploaded_files = st.file_uploader(
            "è«‹é¸æ“‡ JPG/PNG ç…§ç‰‡...", 
            type=['jpg', 'png', 'jpeg'], 
            accept_multiple_files=True, 
            key=f"uploader_{st.session_state.uploader_key}"
        )
        
        if uploaded_files:
            for f in uploaded_files: 
                if not any(x['file'].name == f.name for x in st.session_state.photo_gallery if x['file']):
                    st.session_state.photo_gallery.append({
                        'file': f, 
                        'table_md': None, 
                        'header_text': None,
                        'full_text': None,
                        'raw_json': None
                    })
            st.session_state.uploader_key += 1
            if st.session_state.enable_auto_analysis:
                st.session_state.auto_start_analysis = True
            components.html("""<script>window.parent.document.body.scrollTo(0, window.parent.document.body.scrollHeight);</script>""", height=0)
            st.rerun()
            
    else: 
        st.info("ğŸ’¡ è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•ï¼Œå¾ä½ çš„è³‡æ–™å¤¾é¸æ“‡ä¹‹å‰ä¸‹è¼‰çš„ `.json` æª”ã€‚")
        uploaded_json = st.file_uploader("ä¸Šå‚³JSONæª”", type=['json'], key="json_uploader")
        
        if uploaded_json:
            try:
                current_file_name = uploaded_json.name
                last_loaded_file = st.session_state.get('last_loaded_json_name')

                if current_file_name != last_loaded_file:
                    json_data = json.load(uploaded_json)
                    # ... (å‰æ®µä»£ç¢¼) ...
                    
                    # å¼·åˆ¶é‡ç½®ç›¸ç°¿
                    st.session_state.photo_gallery = []
                    st.session_state.source_mode = 'json'
                    st.session_state.last_loaded_json_name = current_file_name
                    
                    # å¼•å…¥ regex æ¨¡çµ„ (å¦‚æœä¸Šé¢æ²’å¼•ç”¨çš„è©±)
                    import re

                    # é‚„åŸè³‡æ–™
                    for page in json_data:
                        # ã€ä¿®æ”¹é»ã€‘å˜—è©¦å¾ full_text é‡æ–°æŠ“å–çœŸå¯¦é ç¢¼
                        real_page = "Unknown"
                        full_text = page.get('full_text', '')
                        
                        # ä½¿ç”¨è·Ÿ Azure ä¸€æ¨£çš„ Regex æŠ“å– "é …æ¬¡: 3/4"
                        if full_text:
                            match = re.search(r"(?:é …æ¬¡|Page|é æ¬¡|NO\.)[:\s]*(\d+)\s*[/ï¼]\s*\d+", full_text, re.IGNORECASE)
                            if match:
                                real_page = match.group(1)
                        
                        # å¦‚æœ JSON è£¡åŸæœ¬å°±æœ‰å­˜ï¼Œä¹Ÿå¯ä»¥å„ªå…ˆç”¨å­˜çš„
                        # ä½†é‡æŠ“ä¸€æ¬¡æ¯”è¼ƒä¿éšª
                        
                        st.session_state.photo_gallery.append({
                            'file': None,
                            'table_md': page.get('table_md'),
                            'header_text': page.get('header_text'),
                            'full_text': full_text,
                            'raw_json': page.get('raw_json'),
                            'real_page': real_page # <--- æŠŠæŠ“åˆ°çš„é ç¢¼å­˜é€²å»ï¼
                        })
                    
                    # ... (å¾Œæ®µä»£ç¢¼) ...
                    
                    st.toast(f"âœ… æˆåŠŸè¼‰å…¥: {current_file_name}", icon="ğŸ“‚")
                    if st.session_state.enable_auto_analysis:
                        st.session_state.auto_start_analysis = True
                    st.rerun()
                else:
                    st.success(f"ğŸ“‚ ç›®å‰è¼‰å…¥æª”æ¡ˆï¼š**{uploaded_json.name}** (å…± {len(st.session_state.photo_gallery)} é )")
            except Exception as e:
                st.error(f"JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")

if st.session_state.photo_gallery:
    st.caption(f"å·²ç´¯ç© {len(st.session_state.photo_gallery)} é æ–‡ä»¶")
    col_btn1, col_btn2 = st.columns([1, 1], gap="small")
    with col_btn1: start_btn = st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    with col_btn2: 
        clear_btn = st.button("ğŸ—‘ï¸ç…§ç‰‡æ¸…é™¤", help="æ¸…é™¤", use_container_width=True)

    if clear_btn:
        st.session_state.photo_gallery = []
        st.session_state.analysis_result_cache = None
        if 'last_loaded_json_name' in st.session_state:
            del st.session_state.last_loaded_json_name 
        st.rerun()

    is_auto_start = st.session_state.auto_start_analysis
    if is_auto_start:
        st.session_state.auto_start_analysis = False

    if 'analysis_result_cache' not in st.session_state:
        st.session_state.analysis_result_cache = None

    trigger_analysis = start_btn or is_auto_start

    if trigger_analysis:
        total_start = time.time()
        status = st.empty()
        progress_bar = st.progress(0)
        
        extracted_data_list = [None] * len(st.session_state.photo_gallery)
        full_text_for_search = ""
        total_imgs = len(st.session_state.photo_gallery)
        
        ocr_start = time.time()
        
        def process_image_task(index, item):
            index = int(index)
            if item.get('table_md') and item.get('header_text') and item.get('full_text'):
                real_page = item.get('real_page', str(index + 1))
                return index, item['table_md'], item['header_text'], item['full_text'], item.get('raw_json'), real_page, None
            
            try:
                if item.get('file') is None:
                    return index, None, None, None, None, None, "ç„¡åœ–ç‰‡æª”æ¡ˆ"
                
                item['file'].seek(0)
                table_md, header, full, raw, real_page = extract_layout_with_azure(item['file'], DOC_ENDPOINT, DOC_KEY)
                return index, table_md, header, full, raw, real_page, None
            except Exception as e:
                return index, None, None, None, None, None, f"OCRå¤±æ•—: {str(e)}"

        status.text(f"Azure æ­£åœ¨å¹³è¡Œæƒæ {total_imgs} é æ–‡ä»¶...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for i, item in enumerate(st.session_state.photo_gallery):
                futures.append(executor.submit(process_image_task, i, item))
            
            completed_count = 0
            for future in concurrent.futures.as_completed(futures):
                idx, t_md, h_txt, f_txt, raw_j, r_page, err = future.result()
                idx = int(idx)
                
                if err:
                    st.error(f"ç¬¬ {idx+1} é è®€å–å¤±æ•—: {err}")
                    extracted_data_list[idx] = None
                else:
                    st.session_state.photo_gallery[idx]['table_md'] = t_md
                    st.session_state.photo_gallery[idx]['header_text'] = h_txt
                    st.session_state.photo_gallery[idx]['full_text'] = f_txt
                    st.session_state.photo_gallery[idx]['raw_json'] = raw_j
                    st.session_state.photo_gallery[idx]['real_page'] = r_page
                    
                    extracted_data_list[idx] = {
                        "page": r_page,
                        "table": t_md or "", 
                        "header_text": h_txt or ""
                    }
                
                completed_count += 1
                progress_bar.progress(completed_count / (total_imgs + 1))
        
        for i, data in enumerate(extracted_data_list):
            if data and isinstance(data, dict):
                page_idx = i
                if 0 <= page_idx < len(st.session_state.photo_gallery):
                    full_text_for_search += st.session_state.photo_gallery[page_idx].get('full_text', '')

        ocr_end = time.time()
        ocr_duration = ocr_end - ocr_start

        combined_input = "ä»¥ä¸‹æ˜¯å„é è³‡æ–™ï¼š\n"
        for i, data in enumerate(extracted_data_list):
            if data is None: continue
            page_num = data.get('page', i+1)
            table_text = data.get('table', '')
            header_text = data.get('header_text', '')
            combined_input += f"\n=== Page {page_num} ===\nã€é é¦–ã€‘:\n{header_text}\nã€è¡¨æ ¼ã€‘:\n{table_text}\n"
            
        status.text("ç¸½ç¨½æ ¸ Agent æ­£åœ¨é€²è¡Œå…¨æ–¹ä½åˆ†æ...")
        
        # --- å–®ä¸€ä»£ç†åŸ·è¡Œ ---
        t0 = time.time()
        # å‘¼å«åˆä½µå¾Œçš„ Agent
        res_main = agent_unified_check(combined_input, full_text_for_search, GEMINI_KEY, main_model_name)
        t1 = time.time()
        time_main = t1 - t0
        
        progress_bar.progress(100)
        status.empty()
        
        total_end = time.time()
        
        # --- æˆæœ¬è¨ˆç®— (å–®æ¬¡å‘¼å«) ---
        usage_main = res_main.get("_token_usage", {"input": 0, "output": 0})
        
        # è²»ç‡åˆ¤æ–·
        def get_model_rate(model_name):
            name = model_name.lower()
            if "gpt" in name:
                if "mini" in name: return 0.15, 0.60
                elif "3.5" in name: return 0.50, 1.50
                else: return 2.50, 10.00
            else:
                # Gemini è²»ç‡
                if "flash" in name: return 0.075, 0.30
                else: return 1.25, 5.00 # Pro

        rate_in, rate_out = get_model_rate(main_model_name)
        
        cost_usd = (usage_main["input"] / 1_000_000 * rate_in) + (usage_main["output"] / 1_000_000 * rate_out)
        cost_twd = cost_usd * 32.5
        
        # --- Python è¡¨é ­æª¢æŸ¥ ---
        python_header_issues, python_debug_data = python_header_check(st.session_state.photo_gallery)
        
        # --- åˆä½µçµæœ ---
        ai_issues = res_main.get("issues", [])
        for i in ai_issues: 
            i['source'] = 'ğŸ¤– ç¸½ç¨½æ ¸ AI'
            
        all_issues = ai_issues + python_header_issues
        
        st.session_state.analysis_result_cache = {
            "job_no": res_main.get("job_no", "Unknown"),
            "all_issues": all_issues,
            "total_duration": total_end - total_start,
            "cost_twd": cost_twd,
            "total_in": usage_main["input"],
            "total_out": usage_main["output"],
            "ocr_duration": ocr_duration,
            "time_eng": time_main, # é€™è£¡å€Ÿç”¨è®Šæ•¸åï¼Œå¯¦ç‚ºç¸½æ™‚é–“
            "time_acc": 0,         # å–®ä¸€ä»£ç†ç„¡ç¬¬äºŒæ™‚é–“
            "full_text_for_search": full_text_for_search,
            "combined_input": combined_input,
            "python_debug_data": python_debug_data
        }

    if st.session_state.analysis_result_cache:
        cache = st.session_state.analysis_result_cache
        all_issues = cache['all_issues']
        
        st.success(f"å·¥ä»¤: {cache['job_no']} | â±ï¸ {cache['total_duration']:.1f}s")
        st.info(f"ğŸ’° æœ¬æ¬¡æˆæœ¬: NT$ {cache['cost_twd']:.2f} (In: {cache['total_in']:,} / Out: {cache['total_out']:,})")
        st.caption(f"ç´°ç¯€è€—æ™‚: Azure OCR {cache['ocr_duration']:.1f}s | AI åˆ†æ {cache['time_eng']:.1f}s")
        
        with st.expander("ğŸ” æŸ¥çœ‹ AI è®€å–åˆ°çš„ Excel è¦å‰‡ (Debug)"):
            rules_text = get_dynamic_rules(cache['full_text_for_search'], debug_mode=True)
            if "ç„¡ç‰¹å®šè¦å‰‡" in rules_text:
                st.caption("ç„¡åŒ¹é…è¦å‰‡")
            else:
                st.markdown(rules_text)

        with st.expander("ğŸ æŸ¥çœ‹ Python ç¡¬é‚è¼¯åµæ¸¬çµæœ (Debug)", expanded=False):
            if cache.get('python_debug_data'):
                p_data = cache['python_debug_data']
                standard_data = {}
                all_values = {"å·¥ä»¤ç·¨è™Ÿ": [], "é å®šäº¤è²¨": [], "å¯¦éš›äº¤è²¨": []}
                for page in p_data:
                    for k in all_values.keys():
                        if page.get(k) and page[k] != "N/A":
                            all_values[k].append(page[k])
                
                standard_row = {"é ç¢¼": "ğŸ† åˆ¤å®šæ¨™æº–"}
                for k, v in all_values.items():
                    if v:
                        standard_row[k] = Counter(v).most_common(1)[0][0]
                    else:
                        standard_row[k] = "N/A"
                
                final_df_data = [standard_row] + p_data
                st.dataframe(final_df_data, use_container_width=True, hide_index=True)
                st.info("ğŸ’¡ ã€Œåˆ¤å®šæ¨™æº–ã€æ˜¯ä¾æ“šå¤šæ•¸æ±ºç”¢ç”Ÿçš„ã€‚")
            else:
                st.caption("ç„¡åµæ¸¬è³‡æ–™")

        real_errors = [i for i in all_issues if "æœªåŒ¹é…" not in i.get('issue_type', '')]
        
        if not real_errors:
            st.balloons()
            if not all_issues:
                st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
            else:
                st.success(f"âœ… æ•¸å€¼å…¨æ•¸åˆæ ¼ï¼ (ä½†æœ‰ {len(all_issues)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡ï¼Œè«‹æª¢æŸ¥)")
        else:
            st.error(f"ç™¼ç¾ {len(real_errors)} é¡æ•¸å€¼ç•°å¸¸ï¼Œå¦æœ‰ {len(all_issues) - len(real_errors)} å€‹é …ç›®æœªåŒ¹é…è¦å‰‡")

        for item in all_issues:
            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                
                source_label = item.get('source', '')
                rule_source = item.get('rule_used', 'ç³»çµ±é è¨­é‚è¼¯')
                issue_type = item.get('issue_type', 'ç•°å¸¸')
                common_reason = item.get('common_reason', '')
                
                c1.markdown(f"**P.{item.get('page', '?')} | {item.get('item')}**  `{source_label}`")
                
                if "Excel" in rule_source:
                    c1.caption(f"ğŸ“œ åˆ¤æ–·ä¾æ“š: :blue-background[{rule_source}]")
                elif "ç„¡å°æ‡‰" in rule_source or "ç›²æ¸¬" in rule_source:
                    c1.caption(f"âš ï¸ åˆ¤æ–·ä¾æ“š: :grey-background[â“ ç„¡å°æ‡‰è¦å‰‡ (ç›²æ¸¬)]")
                else:
                    c1.caption(f"ğŸ¤– åˆ¤æ–·ä¾æ“š: {rule_source}")
                
                if "æœªåŒ¹é…" in issue_type:
                    if "åˆæ ¼" in common_reason:
                        c2.warning(f"âš ï¸ æœªåŒ¹é…") 
                    else:
                        c2.error(f"ğŸ›‘ æœªåŒ¹é…è¶…è¦") 
                elif "æµç¨‹" in issue_type or "å°ºå¯¸" in issue_type or "çµ±è¨ˆ" in issue_type:
                    c2.error(f"ğŸ›‘ {issue_type}")
                else:
                    c2.warning(f"âš ï¸ {issue_type}")
                
                st.caption(f"åŸå› : {common_reason}")
                
                spec = item.get('spec_logic') or item.get('target_spec')
                if spec: st.caption(f"æ¨™æº–: {spec}")
                
                if item.get('verification_logic'): st.caption(f"é©—è­‰: {item.get('verification_logic')}")
                
                failures = item.get('failures', [])
                if failures:
                    table_data = []
                    for f in failures:
                        if isinstance(f, dict):
                            row = {
                                "æ»¾è¼ªç·¨è™Ÿ": f.get('id', 'æœªçŸ¥'), 
                                "å¯¦æ¸¬/è¨ˆæ•¸": f.get('val', 'N/A')
                            }
                            if f.get('calc'): row["å·®å€¼/å‚™è¨»"] = f.get('calc')
                            if f.get('target'): row["è¦æ ¼/å‚™è¨»"] = f.get('target')
                            table_data.append(row)
                        elif isinstance(f, str):
                            table_data.append({"æ»¾è¼ªç·¨è™Ÿ": "-", "å…§å®¹": f})
                    if table_data:
                        st.dataframe(table_data, use_container_width=True, hide_index=True)
                
                elif 'roll_id' in item:
                    table_data = [{
                        "æ»¾è¼ªç·¨è™Ÿ": item.get('roll_id'),
                        "å¯¦æ¸¬å€¼": item.get('raw_value'),
                        "è¦æ ¼": item.get('target_spec')
                    }]
                    st.dataframe(table_data, use_container_width=True, hide_index=True)
                else:
                    st.text(f"å¯¦æ¸¬æ•¸æ“š: {item.get('measured', 'N/A')}")
        
        st.divider()

        current_job_no = cache.get('job_no', 'Unknown')
        safe_job_no = current_job_no.replace("/", "_").replace("\\", "_").strip()
        file_name_str = f"{safe_job_no}_cleaned.json"

        # æº–å‚™åŒ¯å‡ºè³‡æ–™
        export_data = []
        for item in st.session_state.photo_gallery:
            export_data.append({
                "table_md": item.get('table_md'),
                "header_text": item.get('header_text'),
                "full_text": item.get('full_text'),
                "raw_json": item.get('raw_json')
            })
        json_str = json.dumps(export_data, indent=2, ensure_ascii=False)

        st.subheader("ğŸ’¾ æ¸¬è©¦è³‡æ–™å­˜æª”")
        st.caption(f"å·²è­˜åˆ¥å·¥ä»¤ï¼š**{current_job_no}**ã€‚ä¸‹è¼‰å¾Œå¯ä¾›ä¸‹æ¬¡æ¸¬è©¦ä½¿ç”¨ã€‚")
        
        st.download_button(
            label=f"â¬‡ï¸ ä¸‹è¼‰æ¸¬è©¦è³‡æ–™ ({file_name_str})",
            data=json_str,
            file_name=file_name_str,
            mime="application/json",
            type="primary"
        )

        with st.expander("ğŸ‘€ æŸ¥çœ‹å‚³çµ¦ AI çš„æœ€çµ‚æ–‡å­— (Prompt Input)"):
            st.caption("é€™æ‰æ˜¯ AI çœŸæ­£è®€åˆ°çš„å…§å®¹ (å·²éæ¿¾é›œè¨Š)ï¼š")
            st.code(cache['combined_input'], language='markdown')
            
        st.markdown("### ğŸ” Azure OCR åŸå§‹è³‡æ–™ (Debug)")
        for i, item in enumerate(st.session_state.photo_gallery):
            with st.expander(f"ğŸ“„ ç¬¬ {i+1} é  Raw JSON", expanded=False):
                if item.get('raw_json'):
                    st.json(item.get('raw_json'))
                else:
                    st.caption("å°šæœªå–å¾—è³‡æ–™")
    
    if st.session_state.photo_gallery and st.session_state.get('source_mode') != 'json':
        st.caption("å·²æ‹æ”ç…§ç‰‡ï¼š")
        cols = st.columns(4)
        for idx, item in enumerate(st.session_state.photo_gallery):
            with cols[idx % 4]:
                if item.get('file'):
                    st.image(item['file'], caption=f"P.{idx+1}", use_container_width=True)
                if st.button("âŒ", key=f"del_{idx}"):
                    st.session_state.photo_gallery.pop(idx)
                    st.session_state.analysis_result_cache = None
                    st.rerun()
else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
